(globalThis.TURBOPACK || (globalThis.TURBOPACK = [])).push([typeof document === "object" ? document.currentScript : undefined,
"[project]/node_modules/@langchain/openai/dist/utils/azure.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

/**
 * This function generates an endpoint URL for (Azure) OpenAI
 * based on the configuration parameters provided.
 *
 * @param {OpenAIEndpointConfig} config - The configuration object for the (Azure) endpoint.
 *
 * @property {string} config.azureOpenAIApiDeploymentName - The deployment name of Azure OpenAI.
 * @property {string} config.azureOpenAIApiInstanceName - The instance name of Azure OpenAI, e.g. `example-resource`.
 * @property {string} config.azureOpenAIApiKey - The API Key for Azure OpenAI.
 * @property {string} config.azureOpenAIBasePath - The base path for Azure OpenAI, e.g. `https://example-resource.azure.openai.com/openai/deployments/`.
 * @property {string} config.baseURL - Some other custom base path URL.
 * @property {string} config.azureOpenAIEndpoint - The endpoint for the Azure OpenAI instance, e.g. `https://example-resource.azure.openai.com/`.
 *
 * The function operates as follows:
 * - If both `azureOpenAIBasePath` and `azureOpenAIApiDeploymentName` (plus `azureOpenAIApiKey`) are provided, it returns an URL combining these two parameters (`${azureOpenAIBasePath}/${azureOpenAIApiDeploymentName}`).
 * - If both `azureOpenAIEndpoint` and `azureOpenAIApiDeploymentName` (plus `azureOpenAIApiKey`) are provided, it returns an URL combining these two parameters (`${azureOpenAIEndpoint}/openai/deployments/${azureOpenAIApiDeploymentName}`).
 * - If `azureOpenAIApiKey` is provided, it checks for `azureOpenAIApiInstanceName` and `azureOpenAIApiDeploymentName` and throws an error if any of these is missing. If both are provided, it generates an URL incorporating these parameters.
 * - If none of the above conditions are met, return any custom `baseURL`.
 * - The function returns the generated URL as a string, or undefined if no custom paths are specified.
 *
 * @throws Will throw an error if the necessary parameters for generating the URL are missing.
 *
 * @returns {string | undefined} The generated (Azure) OpenAI endpoint URL.
 */ __turbopack_context__.s([
    "getEndpoint",
    ()=>getEndpoint
]);
function getEndpoint(config) {
    const { azureOpenAIApiDeploymentName, azureOpenAIApiInstanceName, azureOpenAIApiKey, azureOpenAIBasePath, baseURL, azureADTokenProvider, azureOpenAIEndpoint } = config;
    if ((azureOpenAIApiKey || azureADTokenProvider) && azureOpenAIBasePath && azureOpenAIApiDeploymentName) {
        return "".concat(azureOpenAIBasePath, "/").concat(azureOpenAIApiDeploymentName);
    }
    if ((azureOpenAIApiKey || azureADTokenProvider) && azureOpenAIEndpoint && azureOpenAIApiDeploymentName) {
        return "".concat(azureOpenAIEndpoint, "/openai/deployments/").concat(azureOpenAIApiDeploymentName);
    }
    if (azureOpenAIApiKey || azureADTokenProvider) {
        if (!azureOpenAIApiInstanceName) {
            throw new Error("azureOpenAIApiInstanceName is required when using azureOpenAIApiKey");
        }
        if (!azureOpenAIApiDeploymentName) {
            throw new Error("azureOpenAIApiDeploymentName is a required parameter when using azureOpenAIApiKey");
        }
        return "https://".concat(azureOpenAIApiInstanceName, ".openai.azure.com/openai/deployments/").concat(azureOpenAIApiDeploymentName);
    }
    return baseURL;
}
}),
"[project]/node_modules/@langchain/openai/dist/utils/errors.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

/* eslint-disable @typescript-eslint/no-explicit-any */ /* eslint-disable no-param-reassign */ __turbopack_context__.s([
    "addLangChainErrorFields",
    ()=>addLangChainErrorFields
]);
function addLangChainErrorFields(error, lc_error_code) {
    error.lc_error_code = lc_error_code;
    error.message = "".concat(error.message, "\n\nTroubleshooting URL: https://js.langchain.com/docs/troubleshooting/errors/").concat(lc_error_code, "/\n");
    return error;
}
}),
"[project]/node_modules/@langchain/openai/dist/utils/openai.js [app-client] (ecmascript) <locals>", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([
    "formatToOpenAIAssistantTool",
    ()=>formatToOpenAIAssistantTool,
    "formatToOpenAIToolChoice",
    ()=>formatToOpenAIToolChoice,
    "interopZodResponseFormat",
    ()=>interopZodResponseFormat,
    "wrapOpenAIClientError",
    ()=>wrapOpenAIClientError
]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$index$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/openai/index.mjs [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$core$2f$error$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/openai/core/error.mjs [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$function_calling$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/function_calling.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$function_calling$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/function_calling.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$types$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/types.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$types$2f$zod$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/types/zod.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$json_schema$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/json_schema.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$json_schema$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/json_schema.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$zod$2f$v4$2f$core$2f$to$2d$json$2d$schema$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/zod/v4/core/to-json-schema.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$zod$2f$v4$2f$core$2f$parse$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/zod/v4/core/parse.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$helpers$2f$zod$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/openai/helpers/zod.mjs [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$errors$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/errors.js [app-client] (ecmascript)");
;
;
;
;
;
;
;
function wrapOpenAIClientError(e) {
    let error;
    if (e.constructor.name === __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$core$2f$error$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__["APIConnectionTimeoutError"].name) {
        error = new Error(e.message);
        error.name = "TimeoutError";
    } else if (e.constructor.name === __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$core$2f$error$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__["APIUserAbortError"].name) {
        error = new Error(e.message);
        error.name = "AbortError";
    } else if (e.status === 400 && e.message.includes("tool_calls")) {
        error = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$errors$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["addLangChainErrorFields"])(e, "INVALID_TOOL_RESULTS");
    } else if (e.status === 401) {
        error = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$errors$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["addLangChainErrorFields"])(e, "MODEL_AUTHENTICATION");
    } else if (e.status === 429) {
        error = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$errors$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["addLangChainErrorFields"])(e, "MODEL_RATE_LIMIT");
    } else if (e.status === 404) {
        error = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$errors$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["addLangChainErrorFields"])(e, "MODEL_NOT_FOUND");
    } else {
        error = e;
    }
    return error;
}
;
function formatToOpenAIAssistantTool(tool) {
    return {
        type: "function",
        function: {
            name: tool.name,
            description: tool.description,
            parameters: (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$types$2f$zod$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isInteropZodSchema"])(tool.schema) ? (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$json_schema$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["toJsonSchema"])(tool.schema) : tool.schema
        }
    };
}
function formatToOpenAIToolChoice(toolChoice) {
    if (!toolChoice) {
        return undefined;
    } else if (toolChoice === "any" || toolChoice === "required") {
        return "required";
    } else if (toolChoice === "auto") {
        return "auto";
    } else if (toolChoice === "none") {
        return "none";
    } else if (typeof toolChoice === "string") {
        return {
            type: "function",
            function: {
                name: toolChoice
            }
        };
    } else {
        return toolChoice;
    }
}
// inlined from openai/lib/parser.ts
function makeParseableResponseFormat(response_format, parser) {
    const obj = {
        ...response_format
    };
    Object.defineProperties(obj, {
        $brand: {
            value: "auto-parseable-response-format",
            enumerable: false
        },
        $parseRaw: {
            value: parser,
            enumerable: false
        }
    });
    return obj;
}
function interopZodResponseFormat(zodSchema, name, props) {
    if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$types$2f$zod$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isZodSchemaV3"])(zodSchema)) {
        return (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$helpers$2f$zod$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__["zodResponseFormat"])(zodSchema, name, props);
    }
    if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$types$2f$zod$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isZodSchemaV4"])(zodSchema)) {
        return makeParseableResponseFormat({
            type: "json_schema",
            json_schema: {
                ...props,
                name,
                strict: true,
                schema: (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$zod$2f$v4$2f$core$2f$to$2d$json$2d$schema$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["toJSONSchema"])(zodSchema, {
                    cycles: "ref",
                    reused: "ref",
                    override (ctx) {
                        ctx.jsonSchema.title = name; // equivalent to `name` property
                    // TODO: implement `nullableStrategy` patch-fix (zod doesn't support openApi3 json schema target)
                    // TODO: implement `openaiStrictMode` patch-fix (where optional properties without `nullable` are not supported)
                    }
                })
            }
        }, (content)=>(0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$zod$2f$v4$2f$core$2f$parse$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["parse"])(zodSchema, JSON.parse(content)));
    }
    throw new Error("Unsupported schema response format");
}
}),
"[project]/node_modules/@langchain/openai/dist/utils/openai-format-fndef.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([
    "formatFunctionDefinitions",
    ()=>formatFunctionDefinitions
]);
function isAnyOfProp(prop) {
    return prop.anyOf !== undefined && Array.isArray(prop.anyOf);
}
function formatFunctionDefinitions(functions) {
    const lines = [
        "namespace functions {",
        ""
    ];
    for (const f of functions){
        if (f.description) {
            lines.push("// ".concat(f.description));
        }
        var _f_parameters_properties;
        if (Object.keys((_f_parameters_properties = f.parameters.properties) !== null && _f_parameters_properties !== void 0 ? _f_parameters_properties : {}).length > 0) {
            lines.push("type ".concat(f.name, " = (_: {"));
            lines.push(formatObjectProperties(f.parameters, 0));
            lines.push("}) => any;");
        } else {
            lines.push("type ".concat(f.name, " = () => any;"));
        }
        lines.push("");
    }
    lines.push("} // namespace functions");
    return lines.join("\n");
}
// Format just the properties of an object (not including the surrounding braces)
function formatObjectProperties(obj, indent) {
    const lines = [];
    var _obj_properties;
    for (const [name, param] of Object.entries((_obj_properties = obj.properties) !== null && _obj_properties !== void 0 ? _obj_properties : {})){
        var _obj_required;
        if (param.description && indent < 2) {
            lines.push("// ".concat(param.description));
        }
        if ((_obj_required = obj.required) === null || _obj_required === void 0 ? void 0 : _obj_required.includes(name)) {
            lines.push("".concat(name, ": ").concat(formatType(param, indent), ","));
        } else {
            lines.push("".concat(name, "?: ").concat(formatType(param, indent), ","));
        }
    }
    return lines.map((line)=>" ".repeat(indent) + line).join("\n");
}
// Format a single property type
function formatType(param, indent) {
    if (isAnyOfProp(param)) {
        return param.anyOf.map((v)=>formatType(v, indent)).join(" | ");
    }
    switch(param.type){
        case "string":
            if (param.enum) {
                return param.enum.map((v)=>'"'.concat(v, '"')).join(" | ");
            }
            return "string";
        case "number":
            if (param.enum) {
                return param.enum.map((v)=>"".concat(v)).join(" | ");
            }
            return "number";
        case "integer":
            if (param.enum) {
                return param.enum.map((v)=>"".concat(v)).join(" | ");
            }
            return "number";
        case "boolean":
            return "boolean";
        case "null":
            return "null";
        case "object":
            return [
                "{",
                formatObjectProperties(param, indent + 2),
                "}"
            ].join("\n");
        case "array":
            if (param.items) {
                return "".concat(formatType(param.items, indent), "[]");
            }
            return "any[]";
        default:
            return "";
    }
}
}),
"[project]/node_modules/@langchain/openai/dist/utils/tools.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([
    "_convertToOpenAITool",
    ()=>_convertToOpenAITool,
    "convertCompletionsCustomTool",
    ()=>convertCompletionsCustomTool,
    "convertResponsesCustomTool",
    ()=>convertResponsesCustomTool,
    "isBuiltInTool",
    ()=>isBuiltInTool,
    "isBuiltInToolChoice",
    ()=>isBuiltInToolChoice,
    "isCustomTool",
    ()=>isCustomTool,
    "isCustomToolCall",
    ()=>isCustomToolCall,
    "isOpenAICustomTool",
    ()=>isOpenAICustomTool,
    "parseCustomToolCall",
    ()=>parseCustomToolCall
]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$function_calling$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/function_calling.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$function_calling$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/function_calling.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/openai.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$function_calling$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$export__convertToOpenAITool__as__formatToOpenAITool$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/function_calling.js [app-client] (ecmascript) <export convertToOpenAITool as formatToOpenAITool>");
;
;
function _convertToOpenAITool(// eslint-disable-next-line @typescript-eslint/no-explicit-any
tool, fields) {
    let toolDef;
    if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$function_calling$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isLangChainTool"])(tool)) {
        toolDef = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$function_calling$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$export__convertToOpenAITool__as__formatToOpenAITool$3e$__["formatToOpenAITool"])(tool);
    } else {
        toolDef = tool;
    }
    if (toolDef.type === "function" && (fields === null || fields === void 0 ? void 0 : fields.strict) !== undefined) {
        toolDef.function.strict = fields.strict;
    }
    return toolDef;
}
function isBuiltInTool(tool) {
    return "type" in tool && tool.type !== "function" && tool.type !== "custom";
}
function isBuiltInToolChoice(tool_choice) {
    return tool_choice != null && typeof tool_choice === "object" && "type" in tool_choice && tool_choice.type !== "function";
}
function isCustomTool(tool) {
    return typeof tool === "object" && tool !== null && "metadata" in tool && typeof tool.metadata === "object" && tool.metadata !== null && "customTool" in tool.metadata && typeof tool.metadata.customTool === "object" && tool.metadata.customTool !== null;
}
function isOpenAICustomTool(tool) {
    return "type" in tool && tool.type === "custom" && "custom" in tool && typeof tool.custom === "object" && tool.custom !== null;
}
function parseCustomToolCall(// eslint-disable-next-line @typescript-eslint/no-explicit-any
rawToolCall) {
    if (rawToolCall.type !== "custom_tool_call") {
        return undefined;
    }
    return {
        ...rawToolCall,
        type: "tool_call",
        call_id: rawToolCall.id,
        id: rawToolCall.call_id,
        name: rawToolCall.name,
        isCustomTool: true,
        args: {
            input: rawToolCall.input
        }
    };
}
function isCustomToolCall(toolCall) {
    return toolCall.type === "tool_call" && "isCustomTool" in toolCall && toolCall.isCustomTool === true;
}
function convertCompletionsCustomTool(tool) {
    const getFormat = ()=>{
        if (!tool.custom.format) {
            return undefined;
        }
        if (tool.custom.format.type === "grammar") {
            return {
                type: "grammar",
                definition: tool.custom.format.grammar.definition,
                syntax: tool.custom.format.grammar.syntax
            };
        }
        if (tool.custom.format.type === "text") {
            return {
                type: "text"
            };
        }
        return undefined;
    };
    return {
        type: "custom",
        name: tool.custom.name,
        description: tool.custom.description,
        format: getFormat()
    };
}
function convertResponsesCustomTool(tool) {
    const getFormat = ()=>{
        if (!tool.format) {
            return undefined;
        }
        if (tool.format.type === "grammar") {
            return {
                type: "grammar",
                grammar: {
                    definition: tool.format.definition,
                    syntax: tool.format.syntax
                }
            };
        }
        if (tool.format.type === "text") {
            return {
                type: "text"
            };
        }
        return undefined;
    };
    return {
        type: "custom",
        custom: {
            name: tool.name,
            description: tool.description,
            format: getFormat()
        }
    };
}
}),
"[project]/node_modules/@langchain/openai/dist/utils/output.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

/**
 * Handle multi modal response content.
 *
 * @param content The content of the message.
 * @param messages The messages of the response.
 * @returns The new content of the message.
 */ __turbopack_context__.s([
    "handleMultiModalOutput",
    ()=>handleMultiModalOutput
]);
function handleMultiModalOutput(content, messages) {
    /**
     * Handle OpenRouter image responses
     * @see https://openrouter.ai/docs/features/multimodal/image-generation#api-usage
     */ if (messages && typeof messages === "object" && "images" in messages && Array.isArray(messages.images)) {
        const images = messages.images.filter((image)=>{
            var _image_image_url;
            return typeof (image === null || image === void 0 ? void 0 : (_image_image_url = image.image_url) === null || _image_image_url === void 0 ? void 0 : _image_image_url.url) === "string";
        }).map((image)=>({
                type: "image",
                url: image.image_url.url,
                source_type: "url"
            }));
        return [
            {
                type: "text",
                text: content,
                source_type: "text"
            },
            ...images
        ];
    }
    return content;
}
}),
"[project]/node_modules/@langchain/openai/dist/chat_models.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([
    "BaseChatOpenAI",
    ()=>BaseChatOpenAI,
    "ChatOpenAI",
    ()=>ChatOpenAI,
    "ChatOpenAICompletions",
    ()=>ChatOpenAICompletions,
    "ChatOpenAIResponses",
    ()=>ChatOpenAIResponses,
    "_convertMessagesToOpenAIParams",
    ()=>_convertMessagesToOpenAIParams,
    "messageToOpenAIRole",
    ()=>messageToOpenAIRole
]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$index$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/openai/index.mjs [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$client$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/openai/client.mjs [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$messages$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/messages.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$ai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/messages/ai.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$chat$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/messages/chat.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$function$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/messages/function.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$human$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/messages/human.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$system$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/messages/system.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/messages/index.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$content_blocks$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/messages/content_blocks.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$outputs$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/outputs.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$outputs$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/outputs.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/env.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/env.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$language_models$2f$chat_models$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/language_models/chat_models.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$language_models$2f$chat_models$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/language_models/chat_models.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$language_models$2f$base$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/language_models/base.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$language_models$2f$base$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/language_models/base.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$runnables$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/runnables.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$runnables$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/runnables/index.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$output_parsers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/output_parsers.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$json$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/output_parsers/json.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$structured$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/output_parsers/structured.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$output_parsers$2f$openai_tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/output_parsers/openai_tools.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$openai_tools$2f$json_output_tools_parsers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/output_parsers/openai_tools/json_output_tools_parsers.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$types$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/types.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$types$2f$zod$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/types/zod.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$json_schema$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/json_schema.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$json_schema$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/json_schema.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$azure$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/azure.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/openai.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2d$format$2d$fndef$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/openai-format-fndef.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/tools.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$output$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/output.js [app-client] (ecmascript)");
;
;
;
;
;
;
;
;
;
;
;
;
;
;
;
;
const _FUNCTION_CALL_IDS_MAP_KEY = "__openai_function_call_ids__";
function isReasoningModel(model) {
    if (!model) return false;
    if (/^o\d/.test(model !== null && model !== void 0 ? model : "")) return true;
    if (model.startsWith("gpt-5") && !model.startsWith("gpt-5-chat")) return true;
    return false;
}
function isStructuredOutputMethodParams(x) {
    return x !== undefined && // eslint-disable-next-line @typescript-eslint/no-explicit-any
    typeof x.schema === "object";
}
function extractGenericMessageCustomRole(message) {
    if (message.role !== "system" && message.role !== "developer" && message.role !== "assistant" && message.role !== "user" && message.role !== "function" && message.role !== "tool") {
        console.warn("Unknown message role: ".concat(message.role));
    }
    return message.role;
}
function messageToOpenAIRole(message) {
    const type = message._getType();
    switch(type){
        case "system":
            return "system";
        case "ai":
            return "assistant";
        case "human":
            return "user";
        case "function":
            return "function";
        case "tool":
            return "tool";
        case "generic":
            {
                if (!__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$chat$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["ChatMessage"].isInstance(message)) throw new Error("Invalid generic chat message");
                return extractGenericMessageCustomRole(message);
            }
        default:
            throw new Error("Unknown message type: ".concat(type));
    }
}
const completionsApiContentBlockConverter = {
    providerName: "ChatOpenAI",
    fromStandardTextBlock (block) {
        return {
            type: "text",
            text: block.text
        };
    },
    fromStandardImageBlock (block) {
        if (block.source_type === "url") {
            var _block_metadata;
            return {
                type: "image_url",
                image_url: {
                    url: block.url,
                    ...((_block_metadata = block.metadata) === null || _block_metadata === void 0 ? void 0 : _block_metadata.detail) ? {
                        detail: block.metadata.detail
                    } : {}
                }
            };
        }
        if (block.source_type === "base64") {
            var _block_metadata1;
            var _block_mime_type;
            const url = "data:".concat((_block_mime_type = block.mime_type) !== null && _block_mime_type !== void 0 ? _block_mime_type : "", ";base64,").concat(block.data);
            return {
                type: "image_url",
                image_url: {
                    url,
                    ...((_block_metadata1 = block.metadata) === null || _block_metadata1 === void 0 ? void 0 : _block_metadata1.detail) ? {
                        detail: block.metadata.detail
                    } : {}
                }
            };
        }
        throw new Error("Image content blocks with source_type ".concat(block.source_type, " are not supported for ChatOpenAI"));
    },
    fromStandardAudioBlock (block) {
        if (block.source_type === "url") {
            const data = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$content_blocks$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["parseBase64DataUrl"])({
                dataUrl: block.url
            });
            if (!data) {
                throw new Error("URL audio blocks with source_type ".concat(block.source_type, " must be formatted as a data URL for ChatOpenAI"));
            }
            const rawMimeType = data.mime_type || block.mime_type || "";
            let mimeType;
            try {
                mimeType = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$content_blocks$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["parseMimeType"])(rawMimeType);
            } catch (e) {
                throw new Error("Audio blocks with source_type ".concat(block.source_type, " must have mime type of audio/wav or audio/mp3"));
            }
            if (mimeType.type !== "audio" || mimeType.subtype !== "wav" && mimeType.subtype !== "mp3") {
                throw new Error("Audio blocks with source_type ".concat(block.source_type, " must have mime type of audio/wav or audio/mp3"));
            }
            return {
                type: "input_audio",
                input_audio: {
                    format: mimeType.subtype,
                    data: data.data
                }
            };
        }
        if (block.source_type === "base64") {
            let mimeType;
            try {
                var _block_mime_type;
                mimeType = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$content_blocks$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["parseMimeType"])((_block_mime_type = block.mime_type) !== null && _block_mime_type !== void 0 ? _block_mime_type : "");
            } catch (e) {
                throw new Error("Audio blocks with source_type ".concat(block.source_type, " must have mime type of audio/wav or audio/mp3"));
            }
            if (mimeType.type !== "audio" || mimeType.subtype !== "wav" && mimeType.subtype !== "mp3") {
                throw new Error("Audio blocks with source_type ".concat(block.source_type, " must have mime type of audio/wav or audio/mp3"));
            }
            return {
                type: "input_audio",
                input_audio: {
                    format: mimeType.subtype,
                    data: block.data
                }
            };
        }
        throw new Error("Audio content blocks with source_type ".concat(block.source_type, " are not supported for ChatOpenAI"));
    },
    fromStandardFileBlock (block) {
        if (block.source_type === "url") {
            var _block_metadata, _block_metadata1, _block_metadata2, _block_metadata3;
            const data = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$content_blocks$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["parseBase64DataUrl"])({
                dataUrl: block.url
            });
            if (!data) {
                throw new Error("URL file blocks with source_type ".concat(block.source_type, " must be formatted as a data URL for ChatOpenAI"));
            }
            return {
                type: "file",
                file: {
                    file_data: block.url,
                    ...((_block_metadata = block.metadata) === null || _block_metadata === void 0 ? void 0 : _block_metadata.filename) || ((_block_metadata1 = block.metadata) === null || _block_metadata1 === void 0 ? void 0 : _block_metadata1.name) ? {
                        filename: ((_block_metadata2 = block.metadata) === null || _block_metadata2 === void 0 ? void 0 : _block_metadata2.filename) || ((_block_metadata3 = block.metadata) === null || _block_metadata3 === void 0 ? void 0 : _block_metadata3.name)
                    } : {}
                }
            };
        }
        if (block.source_type === "base64") {
            var _block_metadata4, _block_metadata5, _block_metadata6, _block_metadata7, _block_metadata8, _block_metadata9;
            var _block_mime_type;
            return {
                type: "file",
                file: {
                    file_data: "data:".concat((_block_mime_type = block.mime_type) !== null && _block_mime_type !== void 0 ? _block_mime_type : "", ";base64,").concat(block.data),
                    ...((_block_metadata4 = block.metadata) === null || _block_metadata4 === void 0 ? void 0 : _block_metadata4.filename) || ((_block_metadata5 = block.metadata) === null || _block_metadata5 === void 0 ? void 0 : _block_metadata5.name) || ((_block_metadata6 = block.metadata) === null || _block_metadata6 === void 0 ? void 0 : _block_metadata6.title) ? {
                        filename: ((_block_metadata7 = block.metadata) === null || _block_metadata7 === void 0 ? void 0 : _block_metadata7.filename) || ((_block_metadata8 = block.metadata) === null || _block_metadata8 === void 0 ? void 0 : _block_metadata8.name) || ((_block_metadata9 = block.metadata) === null || _block_metadata9 === void 0 ? void 0 : _block_metadata9.title)
                    } : {}
                }
            };
        }
        if (block.source_type === "id") {
            return {
                type: "file",
                file: {
                    file_id: block.id
                }
            };
        }
        throw new Error("File content blocks with source_type ".concat(block.source_type, " are not supported for ChatOpenAI"));
    }
};
function _convertMessagesToOpenAIParams(messages, model) {
    // TODO: Function messages do not support array content, fix cast
    return messages.flatMap((message)=>{
        var _message_tool_calls;
        let role = messageToOpenAIRole(message);
        if (role === "system" && isReasoningModel(model)) {
            role = "developer";
        }
        const content = typeof message.content === "string" ? message.content : message.content.map((m)=>{
            if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$content_blocks$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isDataContentBlock"])(m)) {
                return (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$content_blocks$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["convertToProviderContentBlock"])(m, completionsApiContentBlockConverter);
            }
            return m;
        });
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        const completionParam = {
            role,
            content
        };
        if (message.name != null) {
            completionParam.name = message.name;
        }
        if (message.additional_kwargs.function_call != null) {
            completionParam.function_call = message.additional_kwargs.function_call;
            completionParam.content = "";
        }
        if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$ai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isAIMessage"])(message) && !!((_message_tool_calls = message.tool_calls) === null || _message_tool_calls === void 0 ? void 0 : _message_tool_calls.length)) {
            completionParam.tool_calls = message.tool_calls.map(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$openai_tools$2f$json_output_tools_parsers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["convertLangChainToolCallToOpenAI"]);
            completionParam.content = "";
        } else {
            if (message.additional_kwargs.tool_calls != null) {
                completionParam.tool_calls = message.additional_kwargs.tool_calls;
            }
            if (message.tool_call_id != null) {
                completionParam.tool_call_id = message.tool_call_id;
            }
        }
        if (message.additional_kwargs.audio && typeof message.additional_kwargs.audio === "object" && "id" in message.additional_kwargs.audio) {
            const audioMessage = {
                role: "assistant",
                audio: {
                    id: message.additional_kwargs.audio.id
                }
            };
            return [
                completionParam,
                audioMessage
            ];
        }
        return completionParam;
    });
}
class BaseChatOpenAI extends __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$language_models$2f$chat_models$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["BaseChatModel"] {
    _llmType() {
        return "openai";
    }
    static lc_name() {
        return "ChatOpenAI";
    }
    get callKeys() {
        return [
            ...super.callKeys,
            "options",
            "function_call",
            "functions",
            "tools",
            "tool_choice",
            "promptIndex",
            "response_format",
            "seed",
            "reasoning",
            "reasoningEffort",
            "service_tier"
        ];
    }
    get lc_secrets() {
        return {
            apiKey: "OPENAI_API_KEY",
            organization: "OPENAI_ORGANIZATION"
        };
    }
    get lc_aliases() {
        return {
            apiKey: "openai_api_key",
            modelName: "model",
            reasoningEffort: "reasoning_effort"
        };
    }
    get lc_serializable_keys() {
        return [
            "configuration",
            "logprobs",
            "topLogprobs",
            "prefixMessages",
            "supportsStrictToolCalling",
            "modalities",
            "audio",
            "temperature",
            "maxTokens",
            "topP",
            "frequencyPenalty",
            "presencePenalty",
            "n",
            "logitBias",
            "user",
            "streaming",
            "streamUsage",
            "model",
            "modelName",
            "modelKwargs",
            "stop",
            "stopSequences",
            "timeout",
            "apiKey",
            "cache",
            "maxConcurrency",
            "maxRetries",
            "verbose",
            "callbacks",
            "tags",
            "metadata",
            "disableStreaming",
            "zdrEnabled",
            "reasoning",
            "reasoningEffort",
            "promptCacheKey",
            "verbosity"
        ];
    }
    getLsParams(options) {
        const params = this.invocationParams(options);
        var _params_temperature, _params_max_tokens;
        return {
            ls_provider: "openai",
            ls_model_name: this.model,
            ls_model_type: "chat",
            ls_temperature: (_params_temperature = params.temperature) !== null && _params_temperature !== void 0 ? _params_temperature : undefined,
            ls_max_tokens: (_params_max_tokens = params.max_tokens) !== null && _params_max_tokens !== void 0 ? _params_max_tokens : undefined,
            ls_stop: options.stop
        };
    }
    /** @ignore */ _identifyingParams() {
        return {
            model_name: this.model,
            ...this.invocationParams(),
            ...this.clientConfig
        };
    }
    /**
     * Get the identifying parameters for the model
     */ identifyingParams() {
        return this._identifyingParams();
    }
    /**
     * Returns backwards compatible reasoning parameters from constructor params and call options
     * @internal
     */ _getReasoningParams(options) {
        if (!isReasoningModel(this.model)) {
            return;
        }
        // apply options in reverse order of importance -- newer options supersede older options
        let reasoning;
        if (this.reasoning !== undefined) {
            reasoning = {
                ...reasoning,
                ...this.reasoning
            };
        }
        if ((options === null || options === void 0 ? void 0 : options.reasoning) !== undefined) {
            reasoning = {
                ...reasoning,
                ...options.reasoning
            };
        }
        return reasoning;
    }
    /**
     * Returns an openai compatible response format from a set of options
     * @internal
     */ _getResponseFormat(resFormat) {
        if (resFormat && resFormat.type === "json_schema" && resFormat.json_schema.schema && (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$types$2f$zod$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isInteropZodSchema"])(resFormat.json_schema.schema)) {
            return (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__["interopZodResponseFormat"])(resFormat.json_schema.schema, resFormat.json_schema.name, {
                description: resFormat.json_schema.description
            });
        }
        return resFormat;
    }
    _combineCallOptions(additionalOptions) {
        return {
            ...this.defaultOptions,
            ...additionalOptions !== null && additionalOptions !== void 0 ? additionalOptions : {}
        };
    }
    /** @internal */ _getClientOptions(options) {
        if (!this.client) {
            const openAIEndpointConfig = {
                baseURL: this.clientConfig.baseURL
            };
            const endpoint = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$azure$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEndpoint"])(openAIEndpointConfig);
            const params = {
                ...this.clientConfig,
                baseURL: endpoint,
                timeout: this.timeout,
                maxRetries: 0
            };
            if (!params.baseURL) {
                delete params.baseURL;
            }
            this.client = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$client$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__["OpenAI"](params);
        }
        const requestOptions = {
            ...this.clientConfig,
            ...options
        };
        return requestOptions;
    }
    // TODO: move to completions class
    _convertChatOpenAIToolToCompletionsTool(tool, fields) {
        if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isCustomTool"])(tool)) {
            return (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["convertResponsesCustomTool"])(tool.metadata.customTool);
        }
        if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$language_models$2f$base$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isOpenAITool"])(tool)) {
            if ((fields === null || fields === void 0 ? void 0 : fields.strict) !== undefined) {
                return {
                    ...tool,
                    function: {
                        ...tool.function,
                        strict: fields.strict
                    }
                };
            }
            return tool;
        }
        return (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["_convertToOpenAITool"])(tool, fields);
    }
    bindTools(tools, kwargs) {
        let strict;
        if ((kwargs === null || kwargs === void 0 ? void 0 : kwargs.strict) !== undefined) {
            strict = kwargs.strict;
        } else if (this.supportsStrictToolCalling !== undefined) {
            strict = this.supportsStrictToolCalling;
        }
        return this.withConfig({
            tools: tools.map((tool)=>(0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isBuiltInTool"])(tool) ? tool : this._convertChatOpenAIToolToCompletionsTool(tool, {
                    strict
                })),
            ...kwargs
        });
    }
    async stream(input, options) {
        return super.stream(input, this._combineCallOptions(options));
    }
    async invoke(input, options) {
        return super.invoke(input, this._combineCallOptions(options));
    }
    /** @ignore */ _combineLLMOutput() {
        for(var _len = arguments.length, llmOutputs = new Array(_len), _key = 0; _key < _len; _key++){
            llmOutputs[_key] = arguments[_key];
        }
        return llmOutputs.reduce((acc, llmOutput)=>{
            if (llmOutput && llmOutput.tokenUsage) {
                var _llmOutput_tokenUsage_completionTokens;
                acc.tokenUsage.completionTokens += (_llmOutput_tokenUsage_completionTokens = llmOutput.tokenUsage.completionTokens) !== null && _llmOutput_tokenUsage_completionTokens !== void 0 ? _llmOutput_tokenUsage_completionTokens : 0;
                var _llmOutput_tokenUsage_promptTokens;
                acc.tokenUsage.promptTokens += (_llmOutput_tokenUsage_promptTokens = llmOutput.tokenUsage.promptTokens) !== null && _llmOutput_tokenUsage_promptTokens !== void 0 ? _llmOutput_tokenUsage_promptTokens : 0;
                var _llmOutput_tokenUsage_totalTokens;
                acc.tokenUsage.totalTokens += (_llmOutput_tokenUsage_totalTokens = llmOutput.tokenUsage.totalTokens) !== null && _llmOutput_tokenUsage_totalTokens !== void 0 ? _llmOutput_tokenUsage_totalTokens : 0;
            }
            return acc;
        }, {
            tokenUsage: {
                completionTokens: 0,
                promptTokens: 0,
                totalTokens: 0
            }
        });
    }
    async getNumTokensFromMessages(messages) {
        let totalCount = 0;
        let tokensPerMessage = 0;
        let tokensPerName = 0;
        // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb
        if (this.model === "gpt-3.5-turbo-0301") {
            tokensPerMessage = 4;
            tokensPerName = -1;
        } else {
            tokensPerMessage = 3;
            tokensPerName = 1;
        }
        const countPerMessage = await Promise.all(messages.map(async (message)=>{
            var _openAIMessage_additional_kwargs, _openAIMessage_additional_kwargs_function_call, _openAIMessage_additional_kwargs_function_call1;
            const textCount = await this.getNumTokens(message.content);
            const roleCount = await this.getNumTokens(messageToOpenAIRole(message));
            const nameCount = message.name !== undefined ? tokensPerName + await this.getNumTokens(message.name) : 0;
            let count = textCount + tokensPerMessage + roleCount + nameCount;
            // From: https://github.com/hmarr/openai-chat-tokens/blob/main/src/index.ts messageTokenEstimate
            const openAIMessage = message;
            if (openAIMessage._getType() === "function") {
                count -= 2;
            }
            if ((_openAIMessage_additional_kwargs = openAIMessage.additional_kwargs) === null || _openAIMessage_additional_kwargs === void 0 ? void 0 : _openAIMessage_additional_kwargs.function_call) {
                count += 3;
            }
            if (openAIMessage === null || openAIMessage === void 0 ? void 0 : (_openAIMessage_additional_kwargs_function_call = openAIMessage.additional_kwargs.function_call) === null || _openAIMessage_additional_kwargs_function_call === void 0 ? void 0 : _openAIMessage_additional_kwargs_function_call.name) {
                var _openAIMessage_additional_kwargs_function_call2;
                count += await this.getNumTokens((_openAIMessage_additional_kwargs_function_call2 = openAIMessage.additional_kwargs.function_call) === null || _openAIMessage_additional_kwargs_function_call2 === void 0 ? void 0 : _openAIMessage_additional_kwargs_function_call2.name);
            }
            if ((_openAIMessage_additional_kwargs_function_call1 = openAIMessage.additional_kwargs.function_call) === null || _openAIMessage_additional_kwargs_function_call1 === void 0 ? void 0 : _openAIMessage_additional_kwargs_function_call1.arguments) {
                try {
                    var _openAIMessage_additional_kwargs_function_call3;
                    count += await this.getNumTokens(// Remove newlines and spaces
                    JSON.stringify(JSON.parse((_openAIMessage_additional_kwargs_function_call3 = openAIMessage.additional_kwargs.function_call) === null || _openAIMessage_additional_kwargs_function_call3 === void 0 ? void 0 : _openAIMessage_additional_kwargs_function_call3.arguments)));
                } catch (error) {
                    var _openAIMessage_additional_kwargs_function_call4;
                    console.error("Error parsing function arguments", error, JSON.stringify(openAIMessage.additional_kwargs.function_call));
                    count += await this.getNumTokens((_openAIMessage_additional_kwargs_function_call4 = openAIMessage.additional_kwargs.function_call) === null || _openAIMessage_additional_kwargs_function_call4 === void 0 ? void 0 : _openAIMessage_additional_kwargs_function_call4.arguments);
                }
            }
            totalCount += count;
            return count;
        }));
        totalCount += 3; // every reply is primed with <|start|>assistant<|message|>
        return {
            totalCount,
            countPerMessage
        };
    }
    /** @internal */ async _getNumTokensFromGenerations(generations) {
        const generationUsages = await Promise.all(generations.map(async (generation)=>{
            var _generation_message_additional_kwargs;
            if ((_generation_message_additional_kwargs = generation.message.additional_kwargs) === null || _generation_message_additional_kwargs === void 0 ? void 0 : _generation_message_additional_kwargs.function_call) {
                return (await this.getNumTokensFromMessages([
                    generation.message
                ])).countPerMessage[0];
            } else {
                return await this.getNumTokens(generation.message.content);
            }
        }));
        return generationUsages.reduce((a, b)=>a + b, 0);
    }
    /** @internal */ async _getEstimatedTokenCountFromPrompt(messages, functions, function_call) {
        // It appears that if functions are present, the first system message is padded with a trailing newline. This
        // was inferred by trying lots of combinations of messages and functions and seeing what the token counts were.
        let tokens = (await this.getNumTokensFromMessages(messages)).totalCount;
        // If there are functions, add the function definitions as they count towards token usage
        if (functions && function_call !== "auto") {
            const promptDefinitions = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2d$format$2d$fndef$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["formatFunctionDefinitions"])(functions);
            tokens += await this.getNumTokens(promptDefinitions);
            tokens += 9; // Add nine per completion
        }
        // If there's a system message _and_ functions are present, subtract four tokens. I assume this is because
        // functions typically add a system message, but reuse the first one if it's already there. This offsets
        // the extra 9 tokens added by the function definitions.
        if (functions && messages.find((m)=>m._getType() === "system")) {
            tokens -= 4;
        }
        // If function_call is 'none', add one token.
        // If it's a FunctionCall object, add 4 + the number of tokens in the function name.
        // If it's undefined or 'auto', don't add anything.
        if (function_call === "none") {
            tokens += 1;
        } else if (typeof function_call === "object") {
            tokens += await this.getNumTokens(function_call.name) + 4;
        }
        return tokens;
    }
    /** @internal */ _getStructuredOutputMethod(config) {
        const ensuredConfig = {
            ...config
        };
        if (!this.model.startsWith("gpt-3") && !this.model.startsWith("gpt-4-") && this.model !== "gpt-4") {
            if ((ensuredConfig === null || ensuredConfig === void 0 ? void 0 : ensuredConfig.method) === undefined) {
                return "jsonSchema";
            }
        } else if (ensuredConfig.method === "jsonSchema") {
            console.warn('[WARNING]: JSON Schema is not supported for model "'.concat(this.model, '". Falling back to tool calling.'));
        }
        return ensuredConfig.method;
    }
    withStructuredOutput(outputSchema, config) {
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        let schema;
        let name;
        let method;
        let includeRaw;
        if (isStructuredOutputMethodParams(outputSchema)) {
            schema = outputSchema.schema;
            name = outputSchema.name;
            method = outputSchema.method;
            includeRaw = outputSchema.includeRaw;
        } else {
            schema = outputSchema;
            name = config === null || config === void 0 ? void 0 : config.name;
            method = config === null || config === void 0 ? void 0 : config.method;
            includeRaw = config === null || config === void 0 ? void 0 : config.includeRaw;
        }
        let llm;
        let outputParser;
        if ((config === null || config === void 0 ? void 0 : config.strict) !== undefined && method === "jsonMode") {
            throw new Error("Argument `strict` is only supported for `method` = 'function_calling'");
        }
        method = this._getStructuredOutputMethod({
            ...config,
            method
        });
        if (method === "jsonMode") {
            if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$types$2f$zod$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isInteropZodSchema"])(schema)) {
                outputParser = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$structured$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["StructuredOutputParser"].fromZodSchema(schema);
            } else {
                outputParser = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$json$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["JsonOutputParser"]();
            }
            const asJsonSchema = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$json_schema$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["toJsonSchema"])(schema);
            llm = this.withConfig({
                response_format: {
                    type: "json_object"
                },
                ls_structured_output_format: {
                    kwargs: {
                        method: "json_mode"
                    },
                    schema: {
                        title: name !== null && name !== void 0 ? name : "extract",
                        ...asJsonSchema
                    }
                }
            });
        } else if (method === "jsonSchema") {
            const openaiJsonSchemaParams = {
                name: name !== null && name !== void 0 ? name : "extract",
                description: (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$types$2f$zod$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getSchemaDescription"])(schema),
                schema,
                strict: config === null || config === void 0 ? void 0 : config.strict
            };
            const asJsonSchema = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$json_schema$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["toJsonSchema"])(openaiJsonSchemaParams.schema);
            llm = this.withConfig({
                response_format: {
                    type: "json_schema",
                    json_schema: openaiJsonSchemaParams
                },
                ls_structured_output_format: {
                    kwargs: {
                        method: "json_schema"
                    },
                    schema: {
                        title: openaiJsonSchemaParams.name,
                        description: openaiJsonSchemaParams.description,
                        ...asJsonSchema
                    }
                }
            });
            if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$types$2f$zod$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isInteropZodSchema"])(schema)) {
                const altParser = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$structured$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["StructuredOutputParser"].fromZodSchema(schema);
                outputParser = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$runnables$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["RunnableLambda"].from((aiMessage)=>{
                    if ("parsed" in aiMessage.additional_kwargs) {
                        return aiMessage.additional_kwargs.parsed;
                    }
                    return altParser;
                });
            } else {
                outputParser = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$json$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["JsonOutputParser"]();
            }
        } else {
            let functionName = name !== null && name !== void 0 ? name : "extract";
            // Is function calling
            if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$types$2f$zod$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isInteropZodSchema"])(schema)) {
                const asJsonSchema = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$json_schema$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["toJsonSchema"])(schema);
                llm = this.withConfig({
                    tools: [
                        {
                            type: "function",
                            function: {
                                name: functionName,
                                description: asJsonSchema.description,
                                parameters: asJsonSchema
                            }
                        }
                    ],
                    tool_choice: {
                        type: "function",
                        function: {
                            name: functionName
                        }
                    },
                    ls_structured_output_format: {
                        kwargs: {
                            method: "function_calling"
                        },
                        schema: {
                            title: functionName,
                            ...asJsonSchema
                        }
                    },
                    // Do not pass `strict` argument to OpenAI if `config.strict` is undefined
                    ...(config === null || config === void 0 ? void 0 : config.strict) !== undefined ? {
                        strict: config.strict
                    } : {}
                });
                outputParser = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$openai_tools$2f$json_output_tools_parsers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["JsonOutputKeyToolsParser"]({
                    returnSingle: true,
                    keyName: functionName,
                    zodSchema: schema
                });
            } else {
                let openAIFunctionDefinition;
                if (typeof schema.name === "string" && typeof schema.parameters === "object" && schema.parameters != null) {
                    openAIFunctionDefinition = schema;
                    functionName = schema.name;
                } else {
                    var _schema_title;
                    functionName = (_schema_title = schema.title) !== null && _schema_title !== void 0 ? _schema_title : functionName;
                    var _schema_description;
                    openAIFunctionDefinition = {
                        name: functionName,
                        description: (_schema_description = schema.description) !== null && _schema_description !== void 0 ? _schema_description : "",
                        parameters: schema
                    };
                }
                const asJsonSchema = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$json_schema$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["toJsonSchema"])(schema);
                llm = this.withConfig({
                    tools: [
                        {
                            type: "function",
                            function: openAIFunctionDefinition
                        }
                    ],
                    tool_choice: {
                        type: "function",
                        function: {
                            name: functionName
                        }
                    },
                    ls_structured_output_format: {
                        kwargs: {
                            method: "function_calling"
                        },
                        schema: {
                            title: functionName,
                            ...asJsonSchema
                        }
                    },
                    // Do not pass `strict` argument to OpenAI if `config.strict` is undefined
                    ...(config === null || config === void 0 ? void 0 : config.strict) !== undefined ? {
                        strict: config.strict
                    } : {}
                });
                outputParser = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$openai_tools$2f$json_output_tools_parsers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["JsonOutputKeyToolsParser"]({
                    returnSingle: true,
                    keyName: functionName
                });
            }
        }
        if (!includeRaw) {
            return llm.pipe(outputParser);
        }
        const parserAssign = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$runnables$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["RunnablePassthrough"].assign({
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            parsed: (input, config)=>outputParser.invoke(input.raw, config)
        });
        const parserNone = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$runnables$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["RunnablePassthrough"].assign({
            parsed: ()=>null
        });
        const parsedWithFallback = parserAssign.withFallbacks({
            fallbacks: [
                parserNone
            ]
        });
        return __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$runnables$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["RunnableSequence"].from([
            {
                raw: llm
            },
            parsedWithFallback
        ]);
    }
    constructor(fields){
        var _fields_configuration, _fields_configuration1;
        super(fields !== null && fields !== void 0 ? fields : {});
        Object.defineProperty(this, "temperature", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "topP", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "frequencyPenalty", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "presencePenalty", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "n", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "logitBias", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "model", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "gpt-3.5-turbo"
        });
        Object.defineProperty(this, "modelKwargs", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "stop", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "stopSequences", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "user", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "timeout", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "streaming", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "streamUsage", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: true
        });
        Object.defineProperty(this, "maxTokens", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "logprobs", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "topLogprobs", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "apiKey", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "organization", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "__includeRawResponse", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        /** @internal */ Object.defineProperty(this, "client", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        /** @internal */ Object.defineProperty(this, "clientConfig", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        /**
         * Whether the model supports the `strict` argument when passing in tools.
         * If `undefined` the `strict` argument will not be passed to OpenAI.
         */ Object.defineProperty(this, "supportsStrictToolCalling", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "audio", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "modalities", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "reasoning", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        /**
         * Must be set to `true` in tenancies with Zero Data Retention. Setting to `true` will disable
         * output storage in the Responses API, but this DOES NOT enable Zero Data Retention in your
         * OpenAI organization or project. This must be configured directly with OpenAI.
         *
         * See:
         * https://platform.openai.com/docs/guides/your-data
         * https://platform.openai.com/docs/api-reference/responses/create#responses-create-store
         *
         * @default false
         */ Object.defineProperty(this, "zdrEnabled", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        /**
         * Service tier to use for this request. Can be "auto", "default", or "flex" or "priority".
         * Specifies the service tier for prioritization and latency optimization.
         */ Object.defineProperty(this, "service_tier", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        /**
         * Used by OpenAI to cache responses for similar requests to optimize your cache
         * hit rates.
         * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).
         */ Object.defineProperty(this, "promptCacheKey", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        /**
         * The verbosity of the model's response.
         */ Object.defineProperty(this, "verbosity", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "defaultOptions", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "lc_serializable", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: true
        });
        var _fields_apiKey, _ref;
        this.apiKey = (_ref = (_fields_apiKey = fields === null || fields === void 0 ? void 0 : fields.apiKey) !== null && _fields_apiKey !== void 0 ? _fields_apiKey : fields === null || fields === void 0 ? void 0 : (_fields_configuration = fields.configuration) === null || _fields_configuration === void 0 ? void 0 : _fields_configuration.apiKey) !== null && _ref !== void 0 ? _ref : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("OPENAI_API_KEY");
        var _fields_configuration_organization;
        this.organization = (_fields_configuration_organization = fields === null || fields === void 0 ? void 0 : (_fields_configuration1 = fields.configuration) === null || _fields_configuration1 === void 0 ? void 0 : _fields_configuration1.organization) !== null && _fields_configuration_organization !== void 0 ? _fields_configuration_organization : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("OPENAI_ORGANIZATION");
        var _fields_model, _ref1;
        this.model = (_ref1 = (_fields_model = fields === null || fields === void 0 ? void 0 : fields.model) !== null && _fields_model !== void 0 ? _fields_model : fields === null || fields === void 0 ? void 0 : fields.modelName) !== null && _ref1 !== void 0 ? _ref1 : this.model;
        var _fields_modelKwargs;
        this.modelKwargs = (_fields_modelKwargs = fields === null || fields === void 0 ? void 0 : fields.modelKwargs) !== null && _fields_modelKwargs !== void 0 ? _fields_modelKwargs : {};
        this.timeout = fields === null || fields === void 0 ? void 0 : fields.timeout;
        var _fields_temperature;
        this.temperature = (_fields_temperature = fields === null || fields === void 0 ? void 0 : fields.temperature) !== null && _fields_temperature !== void 0 ? _fields_temperature : this.temperature;
        var _fields_topP;
        this.topP = (_fields_topP = fields === null || fields === void 0 ? void 0 : fields.topP) !== null && _fields_topP !== void 0 ? _fields_topP : this.topP;
        var _fields_frequencyPenalty;
        this.frequencyPenalty = (_fields_frequencyPenalty = fields === null || fields === void 0 ? void 0 : fields.frequencyPenalty) !== null && _fields_frequencyPenalty !== void 0 ? _fields_frequencyPenalty : this.frequencyPenalty;
        var _fields_presencePenalty;
        this.presencePenalty = (_fields_presencePenalty = fields === null || fields === void 0 ? void 0 : fields.presencePenalty) !== null && _fields_presencePenalty !== void 0 ? _fields_presencePenalty : this.presencePenalty;
        this.logprobs = fields === null || fields === void 0 ? void 0 : fields.logprobs;
        this.topLogprobs = fields === null || fields === void 0 ? void 0 : fields.topLogprobs;
        var _fields_n;
        this.n = (_fields_n = fields === null || fields === void 0 ? void 0 : fields.n) !== null && _fields_n !== void 0 ? _fields_n : this.n;
        this.logitBias = fields === null || fields === void 0 ? void 0 : fields.logitBias;
        var _fields_stopSequences;
        this.stop = (_fields_stopSequences = fields === null || fields === void 0 ? void 0 : fields.stopSequences) !== null && _fields_stopSequences !== void 0 ? _fields_stopSequences : fields === null || fields === void 0 ? void 0 : fields.stop;
        this.stopSequences = this.stop;
        this.user = fields === null || fields === void 0 ? void 0 : fields.user;
        this.__includeRawResponse = fields === null || fields === void 0 ? void 0 : fields.__includeRawResponse;
        this.audio = fields === null || fields === void 0 ? void 0 : fields.audio;
        this.modalities = fields === null || fields === void 0 ? void 0 : fields.modalities;
        var _fields_reasoning;
        this.reasoning = ((_fields_reasoning = fields === null || fields === void 0 ? void 0 : fields.reasoning) !== null && _fields_reasoning !== void 0 ? _fields_reasoning : fields === null || fields === void 0 ? void 0 : fields.reasoningEffort) ? {
            effort: fields.reasoningEffort
        } : undefined;
        var _fields_maxCompletionTokens;
        this.maxTokens = (_fields_maxCompletionTokens = fields === null || fields === void 0 ? void 0 : fields.maxCompletionTokens) !== null && _fields_maxCompletionTokens !== void 0 ? _fields_maxCompletionTokens : fields === null || fields === void 0 ? void 0 : fields.maxTokens;
        this.disableStreaming = (fields === null || fields === void 0 ? void 0 : fields.disableStreaming) === true;
        this.streaming = (fields === null || fields === void 0 ? void 0 : fields.streaming) === true;
        var _fields_promptCacheKey;
        this.promptCacheKey = (_fields_promptCacheKey = fields === null || fields === void 0 ? void 0 : fields.promptCacheKey) !== null && _fields_promptCacheKey !== void 0 ? _fields_promptCacheKey : this.promptCacheKey;
        var _fields_verbosity;
        this.verbosity = (_fields_verbosity = fields === null || fields === void 0 ? void 0 : fields.verbosity) !== null && _fields_verbosity !== void 0 ? _fields_verbosity : this.verbosity;
        // disable streaming in BaseChatModel if explicitly disabled
        if ((fields === null || fields === void 0 ? void 0 : fields.streaming) === false) this.disableStreaming = true;
        if (this.disableStreaming === true) this.streaming = false;
        var _fields_streamUsage;
        this.streamUsage = (_fields_streamUsage = fields === null || fields === void 0 ? void 0 : fields.streamUsage) !== null && _fields_streamUsage !== void 0 ? _fields_streamUsage : this.streamUsage;
        if (this.disableStreaming) this.streamUsage = false;
        this.clientConfig = {
            apiKey: this.apiKey,
            organization: this.organization,
            dangerouslyAllowBrowser: true,
            ...fields === null || fields === void 0 ? void 0 : fields.configuration
        };
        // If `supportsStrictToolCalling` is explicitly set, use that value.
        // Else leave undefined so it's not passed to OpenAI.
        if ((fields === null || fields === void 0 ? void 0 : fields.supportsStrictToolCalling) !== undefined) {
            this.supportsStrictToolCalling = fields.supportsStrictToolCalling;
        }
        if ((fields === null || fields === void 0 ? void 0 : fields.service_tier) !== undefined) {
            this.service_tier = fields.service_tier;
        }
        var _fields_zdrEnabled;
        this.zdrEnabled = (_fields_zdrEnabled = fields === null || fields === void 0 ? void 0 : fields.zdrEnabled) !== null && _fields_zdrEnabled !== void 0 ? _fields_zdrEnabled : false;
    }
}
class ChatOpenAIResponses extends BaseChatOpenAI {
    invocationParams(options) {
        var _options_tools;
        let strict;
        if ((options === null || options === void 0 ? void 0 : options.strict) !== undefined) {
            strict = options.strict;
        } else if (this.supportsStrictToolCalling !== undefined) {
            strict = this.supportsStrictToolCalling;
        }
        var _options_promptCacheKey;
        const params = {
            model: this.model,
            temperature: this.temperature,
            top_p: this.topP,
            user: this.user,
            // if include_usage is set or streamUsage then stream must be set to true.
            stream: this.streaming,
            previous_response_id: options === null || options === void 0 ? void 0 : options.previous_response_id,
            truncation: options === null || options === void 0 ? void 0 : options.truncation,
            include: options === null || options === void 0 ? void 0 : options.include,
            tools: (options === null || options === void 0 ? void 0 : (_options_tools = options.tools) === null || _options_tools === void 0 ? void 0 : _options_tools.length) ? this._reduceChatOpenAITools(options.tools, {
                stream: this.streaming,
                strict
            }) : undefined,
            tool_choice: (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isBuiltInToolChoice"])(options === null || options === void 0 ? void 0 : options.tool_choice) ? options === null || options === void 0 ? void 0 : options.tool_choice : (()=>{
                const formatted = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__["formatToOpenAIToolChoice"])(options === null || options === void 0 ? void 0 : options.tool_choice);
                if (typeof formatted === "object" && "type" in formatted) {
                    if (formatted.type === "function") {
                        return {
                            type: "function",
                            name: formatted.function.name
                        };
                    } else if (formatted.type === "allowed_tools") {
                        return {
                            type: "allowed_tools",
                            mode: formatted.allowed_tools.mode,
                            tools: formatted.allowed_tools.tools
                        };
                    } else if (formatted.type === "custom") {
                        return {
                            type: "custom",
                            name: formatted.custom.name
                        };
                    }
                }
                return undefined;
            })(),
            text: (()=>{
                if (options === null || options === void 0 ? void 0 : options.text) return options.text;
                const format = this._getResponseFormat(options === null || options === void 0 ? void 0 : options.response_format);
                if ((format === null || format === void 0 ? void 0 : format.type) === "json_schema") {
                    if (format.json_schema.schema != null) {
                        return {
                            format: {
                                type: "json_schema",
                                schema: format.json_schema.schema,
                                description: format.json_schema.description,
                                name: format.json_schema.name,
                                strict: format.json_schema.strict
                            },
                            verbosity: options === null || options === void 0 ? void 0 : options.verbosity
                        };
                    }
                    return undefined;
                }
                return {
                    format,
                    verbosity: options === null || options === void 0 ? void 0 : options.verbosity
                };
            })(),
            parallel_tool_calls: options === null || options === void 0 ? void 0 : options.parallel_tool_calls,
            max_output_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,
            prompt_cache_key: (_options_promptCacheKey = options === null || options === void 0 ? void 0 : options.promptCacheKey) !== null && _options_promptCacheKey !== void 0 ? _options_promptCacheKey : this.promptCacheKey,
            ...this.zdrEnabled ? {
                store: false
            } : {},
            ...this.modelKwargs
        };
        const reasoning = this._getReasoningParams(options);
        if (reasoning !== undefined) {
            params.reasoning = reasoning;
        }
        return params;
    }
    async _generate(messages, options) {
        const invocationParams = this.invocationParams(options);
        if (invocationParams.stream) {
            var _finalChunk_message;
            const stream = this._streamResponseChunks(messages, options);
            let finalChunk;
            for await (const chunk of stream){
                chunk.message.response_metadata = {
                    ...chunk.generationInfo,
                    ...chunk.message.response_metadata
                };
                var _finalChunk_concat;
                finalChunk = (_finalChunk_concat = finalChunk === null || finalChunk === void 0 ? void 0 : finalChunk.concat(chunk)) !== null && _finalChunk_concat !== void 0 ? _finalChunk_concat : chunk;
            }
            return {
                generations: finalChunk ? [
                    finalChunk
                ] : [],
                llmOutput: {
                    estimatedTokenUsage: finalChunk === null || finalChunk === void 0 ? void 0 : (_finalChunk_message = finalChunk.message) === null || _finalChunk_message === void 0 ? void 0 : _finalChunk_message.usage_metadata
                }
            };
        } else {
            const input = this._convertMessagesToResponsesParams(messages);
            const data = await this.completionWithRetry({
                input,
                ...invocationParams,
                stream: false
            }, {
                signal: options === null || options === void 0 ? void 0 : options.signal,
                ...options === null || options === void 0 ? void 0 : options.options
            });
            return {
                generations: [
                    {
                        text: data.output_text,
                        message: this._convertResponsesMessageToBaseMessage(data)
                    }
                ],
                llmOutput: {
                    id: data.id,
                    estimatedTokenUsage: data.usage ? {
                        promptTokens: data.usage.input_tokens,
                        completionTokens: data.usage.output_tokens,
                        totalTokens: data.usage.total_tokens
                    } : undefined
                }
            };
        }
    }
    async *_streamResponseChunks(messages, options, runManager) {
        const streamIterable = await this.completionWithRetry({
            ...this.invocationParams(options),
            input: this._convertMessagesToResponsesParams(messages),
            stream: true
        }, options);
        for await (const data of streamIterable){
            const chunk = this._convertResponsesDeltaToBaseMessageChunk(data);
            if (chunk == null) continue;
            yield chunk;
            var _options_promptIndex;
            await (runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMNewToken(chunk.text || "", {
                prompt: (_options_promptIndex = options.promptIndex) !== null && _options_promptIndex !== void 0 ? _options_promptIndex : 0,
                completion: 0
            }, undefined, undefined, undefined, {
                chunk
            }));
        }
    }
    async completionWithRetry(request, requestOptions) {
        return this.caller.call(async ()=>{
            const clientOptions = this._getClientOptions(requestOptions);
            try {
                var _request_text_format, _request_text;
                // use parse if dealing with json_schema
                if (((_request_text = request.text) === null || _request_text === void 0 ? void 0 : (_request_text_format = _request_text.format) === null || _request_text_format === void 0 ? void 0 : _request_text_format.type) === "json_schema" && !request.stream) {
                    return await this.client.responses.parse(request, clientOptions);
                }
                return await this.client.responses.create(request, clientOptions);
            } catch (e) {
                const error = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__["wrapOpenAIClientError"])(e);
                throw error;
            }
        });
    }
    /** @internal */ _convertResponsesMessageToBaseMessage(response) {
        if (response.error) {
            // TODO: add support for `addLangChainErrorFields`
            const error = new Error(response.error.message);
            error.name = response.error.code;
            throw error;
        }
        let messageId;
        const content = [];
        const tool_calls = [];
        const invalid_tool_calls = [];
        const response_metadata = {
            model: response.model,
            created_at: response.created_at,
            id: response.id,
            incomplete_details: response.incomplete_details,
            metadata: response.metadata,
            object: response.object,
            status: response.status,
            user: response.user,
            service_tier: response.service_tier,
            // for compatibility with chat completion calls.
            model_name: response.model
        };
        const additional_kwargs = {};
        for (const item of response.output){
            if (item.type === "message") {
                messageId = item.id;
                content.push(...item.content.flatMap((part)=>{
                    if (part.type === "output_text") {
                        if ("parsed" in part && part.parsed != null) {
                            additional_kwargs.parsed = part.parsed;
                        }
                        return {
                            type: "text",
                            text: part.text,
                            annotations: part.annotations
                        };
                    }
                    if (part.type === "refusal") {
                        additional_kwargs.refusal = part.refusal;
                        return [];
                    }
                    return part;
                }));
            } else if (item.type === "function_call") {
                var _additional_kwargs, _FUNCTION_CALL_IDS_MAP_KEY1;
                const fnAdapter = {
                    function: {
                        name: item.name,
                        arguments: item.arguments
                    },
                    id: item.call_id
                };
                try {
                    tool_calls.push((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$openai_tools$2f$json_output_tools_parsers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["parseToolCall"])(fnAdapter, {
                        returnId: true
                    }));
                } catch (e) {
                    let errMessage;
                    if (typeof e === "object" && e != null && "message" in e && typeof e.message === "string") {
                        errMessage = e.message;
                    }
                    invalid_tool_calls.push((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$openai_tools$2f$json_output_tools_parsers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["makeInvalidToolCall"])(fnAdapter, errMessage));
                }
                var _;
                (_ = (_additional_kwargs = additional_kwargs)[_FUNCTION_CALL_IDS_MAP_KEY1 = _FUNCTION_CALL_IDS_MAP_KEY]) !== null && _ !== void 0 ? _ : _additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY1] = {};
                if (item.id) {
                    additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY][item.call_id] = item.id;
                }
            } else if (item.type === "reasoning") {
                additional_kwargs.reasoning = item;
            } else if (item.type === "custom_tool_call") {
                const parsed = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["parseCustomToolCall"])(item);
                if (parsed) {
                    tool_calls.push(parsed);
                } else {
                    invalid_tool_calls.push((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$openai_tools$2f$json_output_tools_parsers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["makeInvalidToolCall"])(item, "Malformed custom tool call"));
                }
            } else {
                var _additional_kwargs1;
                var _tool_outputs;
                (_tool_outputs = (_additional_kwargs1 = additional_kwargs).tool_outputs) !== null && _tool_outputs !== void 0 ? _tool_outputs : _additional_kwargs1.tool_outputs = [];
                additional_kwargs.tool_outputs.push(item);
            }
        }
        return new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$ai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["AIMessage"]({
            id: messageId,
            content,
            tool_calls,
            invalid_tool_calls,
            usage_metadata: response.usage,
            additional_kwargs,
            response_metadata
        });
    }
    /** @internal */ _convertResponsesDeltaToBaseMessageChunk(chunk) {
        const content = [];
        let generationInfo = {};
        let usage_metadata;
        const tool_call_chunks = [];
        const response_metadata = {};
        const additional_kwargs = {};
        let id;
        if (chunk.type === "response.output_text.delta") {
            content.push({
                type: "text",
                text: chunk.delta,
                index: chunk.content_index
            });
        } else if (chunk.type === "response.output_text.annotation.added") {
            content.push({
                type: "text",
                text: "",
                annotations: [
                    chunk.annotation
                ],
                index: chunk.content_index
            });
        } else if (chunk.type === "response.output_item.added" && chunk.item.type === "message") {
            id = chunk.item.id;
        } else if (chunk.type === "response.output_item.added" && chunk.item.type === "function_call") {
            tool_call_chunks.push({
                type: "tool_call_chunk",
                name: chunk.item.name,
                args: chunk.item.arguments,
                id: chunk.item.call_id,
                index: chunk.output_index
            });
            additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY] = {
                [chunk.item.call_id]: chunk.item.id
            };
        } else if (chunk.type === "response.output_item.done" && [
            "web_search_call",
            "file_search_call",
            "computer_call",
            "code_interpreter_call",
            "mcp_call",
            "mcp_list_tools",
            "mcp_approval_request",
            "image_generation_call",
            "custom_tool_call"
        ].includes(chunk.item.type)) {
            additional_kwargs.tool_outputs = [
                chunk.item
            ];
        } else if (chunk.type === "response.created") {
            response_metadata.id = chunk.response.id;
            response_metadata.model_name = chunk.response.model;
            response_metadata.model = chunk.response.model;
        } else if (chunk.type === "response.completed") {
            var _chunk_response_text_format, _chunk_response_text;
            const msg = this._convertResponsesMessageToBaseMessage(chunk.response);
            usage_metadata = chunk.response.usage;
            if (((_chunk_response_text = chunk.response.text) === null || _chunk_response_text === void 0 ? void 0 : (_chunk_response_text_format = _chunk_response_text.format) === null || _chunk_response_text_format === void 0 ? void 0 : _chunk_response_text_format.type) === "json_schema") {
                var _additional_kwargs;
                var _parsed;
                (_parsed = (_additional_kwargs = additional_kwargs).parsed) !== null && _parsed !== void 0 ? _parsed : _additional_kwargs.parsed = JSON.parse(msg.text);
            }
            for (const [key, value] of Object.entries(chunk.response)){
                if (key !== "id") response_metadata[key] = value;
            }
        } else if (chunk.type === "response.function_call_arguments.delta" || chunk.type === "response.custom_tool_call_input.delta") {
            tool_call_chunks.push({
                type: "tool_call_chunk",
                args: chunk.delta,
                index: chunk.output_index
            });
        } else if (chunk.type === "response.web_search_call.completed" || chunk.type === "response.file_search_call.completed") {
            generationInfo = {
                tool_outputs: {
                    id: chunk.item_id,
                    type: chunk.type.replace("response.", "").replace(".completed", ""),
                    status: "completed"
                }
            };
        } else if (chunk.type === "response.refusal.done") {
            additional_kwargs.refusal = chunk.refusal;
        } else if (chunk.type === "response.output_item.added" && "item" in chunk && chunk.item.type === "reasoning") {
            const summary = chunk.item.summary ? chunk.item.summary.map((s, index)=>({
                    ...s,
                    index
                })) : undefined;
            additional_kwargs.reasoning = {
                // We only capture ID in the first chunk or else the concatenated result of all chunks will
                // have an ID field that is repeated once per chunk. There is special handling for the `type`
                // field that prevents this, however.
                id: chunk.item.id,
                type: chunk.item.type,
                ...summary ? {
                    summary
                } : {}
            };
        } else if (chunk.type === "response.reasoning_summary_part.added") {
            additional_kwargs.reasoning = {
                type: "reasoning",
                summary: [
                    {
                        ...chunk.part,
                        index: chunk.summary_index
                    }
                ]
            };
        } else if (chunk.type === "response.reasoning_summary_text.delta") {
            additional_kwargs.reasoning = {
                type: "reasoning",
                summary: [
                    {
                        text: chunk.delta,
                        type: "summary_text",
                        index: chunk.summary_index
                    }
                ]
            };
        } else if (chunk.type === "response.image_generation_call.partial_image") {
            // noop/fixme: retaining partial images in a message chunk means that _all_
            // partial images get kept in history, so we don't do anything here.
            return null;
        } else {
            return null;
        }
        return new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$outputs$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["ChatGenerationChunk"]({
            // Legacy reasons, `onLLMNewToken` should pulls this out
            text: content.map((part)=>part.text).join(""),
            message: new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$ai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["AIMessageChunk"]({
                id,
                content,
                tool_call_chunks,
                usage_metadata,
                additional_kwargs,
                response_metadata
            }),
            generationInfo
        });
    }
    /** @internal */ _convertMessagesToResponsesParams(messages) {
        return messages.flatMap((lcMsg)=>{
            const additional_kwargs = lcMsg.additional_kwargs;
            let role = messageToOpenAIRole(lcMsg);
            if (role === "system" && isReasoningModel(this.model)) role = "developer";
            if (role === "function") {
                throw new Error("Function messages are not supported in Responses API");
            }
            if (role === "tool") {
                var _toolMessage_metadata, _toolMessage_id;
                const toolMessage = lcMsg;
                // Handle computer call output
                if ((additional_kwargs === null || additional_kwargs === void 0 ? void 0 : additional_kwargs.type) === "computer_call_output") {
                    const output = (()=>{
                        if (typeof toolMessage.content === "string") {
                            return {
                                type: "computer_screenshot",
                                image_url: toolMessage.content
                            };
                        }
                        if (Array.isArray(toolMessage.content)) {
                            const oaiScreenshot = toolMessage.content.find((i)=>i.type === "computer_screenshot");
                            if (oaiScreenshot) return oaiScreenshot;
                            const lcImage = toolMessage.content.find((i)=>i.type === "image_url");
                            if (lcImage) {
                                return {
                                    type: "computer_screenshot",
                                    image_url: typeof lcImage.image_url === "string" ? lcImage.image_url : lcImage.image_url.url
                                };
                            }
                        }
                        throw new Error("Invalid computer call output");
                    })();
                    return {
                        type: "computer_call_output",
                        output,
                        call_id: toolMessage.tool_call_id
                    };
                }
                // Handle custom tool output
                if ((_toolMessage_metadata = toolMessage.metadata) === null || _toolMessage_metadata === void 0 ? void 0 : _toolMessage_metadata.customTool) {
                    return {
                        type: "custom_tool_call_output",
                        call_id: toolMessage.tool_call_id,
                        output: toolMessage.content
                    };
                }
                return {
                    type: "function_call_output",
                    call_id: toolMessage.tool_call_id,
                    id: ((_toolMessage_id = toolMessage.id) === null || _toolMessage_id === void 0 ? void 0 : _toolMessage_id.startsWith("fc_")) ? toolMessage.id : undefined,
                    output: typeof toolMessage.content !== "string" ? JSON.stringify(toolMessage.content) : toolMessage.content
                };
            }
            if (role === "assistant") {
                var _lcMsg_tool_calls, _lcMsg_response_metadata_output;
                // if we have the original response items, just reuse them
                if (!this.zdrEnabled && lcMsg.response_metadata.output != null && Array.isArray(lcMsg.response_metadata.output) && lcMsg.response_metadata.output.length > 0 && lcMsg.response_metadata.output.every((item)=>"type" in item)) {
                    return lcMsg.response_metadata.output;
                }
                // otherwise, try to reconstruct the response from what we have
                const input = [];
                // reasoning items
                if ((additional_kwargs === null || additional_kwargs === void 0 ? void 0 : additional_kwargs.reasoning) && !this.zdrEnabled) {
                    const reasoningItem = this._convertReasoningSummary(additional_kwargs.reasoning);
                    input.push(reasoningItem);
                }
                // ai content
                let { content } = lcMsg;
                if (additional_kwargs === null || additional_kwargs === void 0 ? void 0 : additional_kwargs.refusal) {
                    if (typeof content === "string") {
                        content = [
                            {
                                type: "output_text",
                                text: content,
                                annotations: []
                            }
                        ];
                    }
                    content = [
                        ...content,
                        {
                            type: "refusal",
                            refusal: additional_kwargs.refusal
                        }
                    ];
                }
                if (typeof content === "string" || content.length > 0) {
                    input.push({
                        type: "message",
                        role: "assistant",
                        ...lcMsg.id && !this.zdrEnabled && lcMsg.id.startsWith("msg_") ? {
                            id: lcMsg.id
                        } : {},
                        content: typeof content === "string" ? content : content.flatMap((item)=>{
                            if (item.type === "text") {
                                var _item_annotations;
                                return {
                                    type: "output_text",
                                    text: item.text,
                                    // @ts-expect-error TODO: add types for `annotations`
                                    annotations: (_item_annotations = item.annotations) !== null && _item_annotations !== void 0 ? _item_annotations : []
                                };
                            }
                            if (item.type === "output_text" || item.type === "refusal") {
                                return item;
                            }
                            return [];
                        })
                    });
                }
                const functionCallIds = additional_kwargs === null || additional_kwargs === void 0 ? void 0 : additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY];
                if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$ai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isAIMessage"])(lcMsg) && !!((_lcMsg_tool_calls = lcMsg.tool_calls) === null || _lcMsg_tool_calls === void 0 ? void 0 : _lcMsg_tool_calls.length)) {
                    input.push(...lcMsg.tool_calls.map((toolCall)=>{
                        if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isCustomToolCall"])(toolCall)) {
                            var _toolCall_id;
                            return {
                                type: "custom_tool_call",
                                id: toolCall.call_id,
                                call_id: (_toolCall_id = toolCall.id) !== null && _toolCall_id !== void 0 ? _toolCall_id : "",
                                input: toolCall.args.input,
                                name: toolCall.name
                            };
                        }
                        return {
                            type: "function_call",
                            name: toolCall.name,
                            arguments: JSON.stringify(toolCall.args),
                            call_id: toolCall.id,
                            ...this.zdrEnabled ? {
                                id: functionCallIds === null || functionCallIds === void 0 ? void 0 : functionCallIds[toolCall.id]
                            } : {}
                        };
                    }));
                } else if (additional_kwargs === null || additional_kwargs === void 0 ? void 0 : additional_kwargs.tool_calls) {
                    input.push(...additional_kwargs.tool_calls.map((toolCall)=>({
                            type: "function_call",
                            name: toolCall.function.name,
                            call_id: toolCall.id,
                            arguments: toolCall.function.arguments,
                            ...this.zdrEnabled ? {
                                id: functionCallIds === null || functionCallIds === void 0 ? void 0 : functionCallIds[toolCall.id]
                            } : {}
                        })));
                }
                const toolOutputs = ((_lcMsg_response_metadata_output = lcMsg.response_metadata.output) === null || _lcMsg_response_metadata_output === void 0 ? void 0 : _lcMsg_response_metadata_output.length) ? lcMsg.response_metadata.output : additional_kwargs.tool_outputs;
                const fallthroughCallTypes = [
                    "computer_call",
                    "mcp_call",
                    "code_interpreter_call",
                    "image_generation_call"
                ];
                if (toolOutputs != null) {
                    const castToolOutputs = toolOutputs;
                    const fallthroughCalls = castToolOutputs === null || castToolOutputs === void 0 ? void 0 : castToolOutputs.filter((item)=>fallthroughCallTypes.includes(item.type));
                    if (fallthroughCalls.length > 0) input.push(...fallthroughCalls);
                }
                return input;
            }
            if (role === "user" || role === "system" || role === "developer") {
                if (typeof lcMsg.content === "string") {
                    return {
                        type: "message",
                        role,
                        content: lcMsg.content
                    };
                }
                const messages = [];
                const content = lcMsg.content.flatMap((item)=>{
                    if (item.type === "mcp_approval_response") {
                        messages.push({
                            type: "mcp_approval_response",
                            approval_request_id: item.approval_request_id,
                            approve: item.approve
                        });
                    }
                    if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$content_blocks$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isDataContentBlock"])(item)) {
                        return (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$content_blocks$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["convertToProviderContentBlock"])(item, completionsApiContentBlockConverter);
                    }
                    if (item.type === "text") {
                        return {
                            type: "input_text",
                            text: item.text
                        };
                    }
                    if (item.type === "image_url") {
                        return {
                            type: "input_image",
                            image_url: typeof item.image_url === "string" ? item.image_url : item.image_url.url,
                            detail: typeof item.image_url === "string" ? "auto" : item.image_url.detail
                        };
                    }
                    if (item.type === "input_text" || item.type === "input_image" || item.type === "input_file") {
                        return item;
                    }
                    return [];
                });
                if (content.length > 0) {
                    messages.push({
                        type: "message",
                        role,
                        content
                    });
                }
                return messages;
            }
            console.warn("Unsupported role found when converting to OpenAI Responses API: ".concat(role));
            return [];
        });
    }
    /** @internal */ _convertReasoningSummary(reasoning) {
        // combine summary parts that have the the same index and then remove the indexes
        const summary = (reasoning.summary.length > 1 ? reasoning.summary.reduce((acc, curr)=>{
            const last = acc.at(-1);
            if (last.index === curr.index) {
                last.text += curr.text;
            } else {
                acc.push(curr);
            }
            return acc;
        }, [
            {
                ...reasoning.summary[0]
            }
        ]) : reasoning.summary).map((s)=>Object.fromEntries(Object.entries(s).filter((param)=>{
                let [k] = param;
                return k !== "index";
            })));
        return {
            ...reasoning,
            summary
        };
    }
    /** @internal */ _reduceChatOpenAITools(tools, fields) {
        const reducedTools = [];
        for (const tool of tools){
            if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isBuiltInTool"])(tool)) {
                if (tool.type === "image_generation" && (fields === null || fields === void 0 ? void 0 : fields.stream)) {
                    // OpenAI sends a 400 error if partial_images is not set and we want to stream.
                    // We also set it to 1 since we don't support partial images yet.
                    tool.partial_images = 1;
                }
                reducedTools.push(tool);
            } else if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$language_models$2f$base$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isOpenAITool"])(tool)) {
                var _fields_strict;
                reducedTools.push({
                    type: "function",
                    name: tool.function.name,
                    parameters: tool.function.parameters,
                    description: tool.function.description,
                    strict: (_fields_strict = fields === null || fields === void 0 ? void 0 : fields.strict) !== null && _fields_strict !== void 0 ? _fields_strict : null
                });
            } else if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isOpenAICustomTool"])(tool)) {
                reducedTools.push((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["convertCompletionsCustomTool"])(tool));
            }
        }
        return reducedTools;
    }
}
class ChatOpenAICompletions extends BaseChatOpenAI {
    /** @internal */ invocationParams(options, extra) {
        var _options_tools;
        let strict;
        if ((options === null || options === void 0 ? void 0 : options.strict) !== undefined) {
            strict = options.strict;
        } else if (this.supportsStrictToolCalling !== undefined) {
            strict = this.supportsStrictToolCalling;
        }
        let streamOptionsConfig = {};
        if ((options === null || options === void 0 ? void 0 : options.stream_options) !== undefined) {
            streamOptionsConfig = {
                stream_options: options.stream_options
            };
        } else if (this.streamUsage && (this.streaming || (extra === null || extra === void 0 ? void 0 : extra.streaming))) {
            streamOptionsConfig = {
                stream_options: {
                    include_usage: true
                }
            };
        }
        var _options_stop, _options_promptCacheKey, _options_verbosity;
        const params = {
            model: this.model,
            temperature: this.temperature,
            top_p: this.topP,
            frequency_penalty: this.frequencyPenalty,
            presence_penalty: this.presencePenalty,
            logprobs: this.logprobs,
            top_logprobs: this.topLogprobs,
            n: this.n,
            logit_bias: this.logitBias,
            stop: (_options_stop = options === null || options === void 0 ? void 0 : options.stop) !== null && _options_stop !== void 0 ? _options_stop : this.stopSequences,
            user: this.user,
            // if include_usage is set or streamUsage then stream must be set to true.
            stream: this.streaming,
            functions: options === null || options === void 0 ? void 0 : options.functions,
            function_call: options === null || options === void 0 ? void 0 : options.function_call,
            tools: (options === null || options === void 0 ? void 0 : (_options_tools = options.tools) === null || _options_tools === void 0 ? void 0 : _options_tools.length) ? options.tools.map((tool)=>this._convertChatOpenAIToolToCompletionsTool(tool, {
                    strict
                })) : undefined,
            tool_choice: (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__["formatToOpenAIToolChoice"])(options === null || options === void 0 ? void 0 : options.tool_choice),
            response_format: this._getResponseFormat(options === null || options === void 0 ? void 0 : options.response_format),
            seed: options === null || options === void 0 ? void 0 : options.seed,
            ...streamOptionsConfig,
            parallel_tool_calls: options === null || options === void 0 ? void 0 : options.parallel_tool_calls,
            ...this.audio || (options === null || options === void 0 ? void 0 : options.audio) ? {
                audio: this.audio || (options === null || options === void 0 ? void 0 : options.audio)
            } : {},
            ...this.modalities || (options === null || options === void 0 ? void 0 : options.modalities) ? {
                modalities: this.modalities || (options === null || options === void 0 ? void 0 : options.modalities)
            } : {},
            ...this.modelKwargs,
            prompt_cache_key: (_options_promptCacheKey = options === null || options === void 0 ? void 0 : options.promptCacheKey) !== null && _options_promptCacheKey !== void 0 ? _options_promptCacheKey : this.promptCacheKey,
            verbosity: (_options_verbosity = options === null || options === void 0 ? void 0 : options.verbosity) !== null && _options_verbosity !== void 0 ? _options_verbosity : this.verbosity
        };
        if ((options === null || options === void 0 ? void 0 : options.prediction) !== undefined) {
            params.prediction = options.prediction;
        }
        if (this.service_tier !== undefined) {
            params.service_tier = this.service_tier;
        }
        if ((options === null || options === void 0 ? void 0 : options.service_tier) !== undefined) {
            params.service_tier = options.service_tier;
        }
        const reasoning = this._getReasoningParams(options);
        if (reasoning !== undefined && reasoning.effort !== undefined) {
            params.reasoning_effort = reasoning.effort;
        }
        if (isReasoningModel(params.model)) {
            params.max_completion_tokens = this.maxTokens === -1 ? undefined : this.maxTokens;
        } else {
            params.max_tokens = this.maxTokens === -1 ? undefined : this.maxTokens;
        }
        return params;
    }
    async _generate(messages, options, runManager) {
        const usageMetadata = {};
        const params = this.invocationParams(options);
        const messagesMapped = _convertMessagesToOpenAIParams(messages, this.model);
        if (params.stream) {
            const stream = this._streamResponseChunks(messages, options, runManager);
            const finalChunks = {};
            for await (const chunk of stream){
                var _chunk_generationInfo;
                chunk.message.response_metadata = {
                    ...chunk.generationInfo,
                    ...chunk.message.response_metadata
                };
                var _chunk_generationInfo_completion;
                const index = (_chunk_generationInfo_completion = (_chunk_generationInfo = chunk.generationInfo) === null || _chunk_generationInfo === void 0 ? void 0 : _chunk_generationInfo.completion) !== null && _chunk_generationInfo_completion !== void 0 ? _chunk_generationInfo_completion : 0;
                if (finalChunks[index] === undefined) {
                    finalChunks[index] = chunk;
                } else {
                    finalChunks[index] = finalChunks[index].concat(chunk);
                }
            }
            const generations = Object.entries(finalChunks).sort((param, param1)=>{
                let [aKey] = param, [bKey] = param1;
                return parseInt(aKey, 10) - parseInt(bKey, 10);
            }).map((param)=>{
                let [_, value] = param;
                return value;
            });
            const { functions, function_call } = this.invocationParams(options);
            // OpenAI does not support token usage report under stream mode,
            // fallback to estimation.
            const promptTokenUsage = await this._getEstimatedTokenCountFromPrompt(messages, functions, function_call);
            const completionTokenUsage = await this._getNumTokensFromGenerations(generations);
            usageMetadata.input_tokens = promptTokenUsage;
            usageMetadata.output_tokens = completionTokenUsage;
            usageMetadata.total_tokens = promptTokenUsage + completionTokenUsage;
            return {
                generations,
                llmOutput: {
                    estimatedTokenUsage: {
                        promptTokens: usageMetadata.input_tokens,
                        completionTokens: usageMetadata.output_tokens,
                        totalTokens: usageMetadata.total_tokens
                    }
                }
            };
        } else {
            const data = await this.completionWithRetry({
                ...params,
                stream: false,
                messages: messagesMapped
            }, {
                signal: options === null || options === void 0 ? void 0 : options.signal,
                ...options === null || options === void 0 ? void 0 : options.options
            });
            var _data_usage;
            const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens, prompt_tokens_details: promptTokensDetails, completion_tokens_details: completionTokensDetails } = (_data_usage = data === null || data === void 0 ? void 0 : data.usage) !== null && _data_usage !== void 0 ? _data_usage : {};
            if (completionTokens) {
                var _usageMetadata_output_tokens;
                usageMetadata.output_tokens = ((_usageMetadata_output_tokens = usageMetadata.output_tokens) !== null && _usageMetadata_output_tokens !== void 0 ? _usageMetadata_output_tokens : 0) + completionTokens;
            }
            if (promptTokens) {
                var _usageMetadata_input_tokens;
                usageMetadata.input_tokens = ((_usageMetadata_input_tokens = usageMetadata.input_tokens) !== null && _usageMetadata_input_tokens !== void 0 ? _usageMetadata_input_tokens : 0) + promptTokens;
            }
            if (totalTokens) {
                var _usageMetadata_total_tokens;
                usageMetadata.total_tokens = ((_usageMetadata_total_tokens = usageMetadata.total_tokens) !== null && _usageMetadata_total_tokens !== void 0 ? _usageMetadata_total_tokens : 0) + totalTokens;
            }
            if ((promptTokensDetails === null || promptTokensDetails === void 0 ? void 0 : promptTokensDetails.audio_tokens) !== null || (promptTokensDetails === null || promptTokensDetails === void 0 ? void 0 : promptTokensDetails.cached_tokens) !== null) {
                usageMetadata.input_token_details = {
                    ...(promptTokensDetails === null || promptTokensDetails === void 0 ? void 0 : promptTokensDetails.audio_tokens) !== null && {
                        audio: promptTokensDetails === null || promptTokensDetails === void 0 ? void 0 : promptTokensDetails.audio_tokens
                    },
                    ...(promptTokensDetails === null || promptTokensDetails === void 0 ? void 0 : promptTokensDetails.cached_tokens) !== null && {
                        cache_read: promptTokensDetails === null || promptTokensDetails === void 0 ? void 0 : promptTokensDetails.cached_tokens
                    }
                };
            }
            if ((completionTokensDetails === null || completionTokensDetails === void 0 ? void 0 : completionTokensDetails.audio_tokens) !== null || (completionTokensDetails === null || completionTokensDetails === void 0 ? void 0 : completionTokensDetails.reasoning_tokens) !== null) {
                usageMetadata.output_token_details = {
                    ...(completionTokensDetails === null || completionTokensDetails === void 0 ? void 0 : completionTokensDetails.audio_tokens) !== null && {
                        audio: completionTokensDetails === null || completionTokensDetails === void 0 ? void 0 : completionTokensDetails.audio_tokens
                    },
                    ...(completionTokensDetails === null || completionTokensDetails === void 0 ? void 0 : completionTokensDetails.reasoning_tokens) !== null && {
                        reasoning: completionTokensDetails === null || completionTokensDetails === void 0 ? void 0 : completionTokensDetails.reasoning_tokens
                    }
                };
            }
            const generations = [];
            var _data_choices;
            for (const part of (_data_choices = data === null || data === void 0 ? void 0 : data.choices) !== null && _data_choices !== void 0 ? _data_choices : []){
                var _part_message;
                var _part_message_content;
                const text = (_part_message_content = (_part_message = part.message) === null || _part_message === void 0 ? void 0 : _part_message.content) !== null && _part_message_content !== void 0 ? _part_message_content : "";
                var _part_message1;
                const generation = {
                    text,
                    message: this._convertCompletionsMessageToBaseMessage((_part_message1 = part.message) !== null && _part_message1 !== void 0 ? _part_message1 : {
                        role: "assistant"
                    }, data)
                };
                generation.generationInfo = {
                    ...part.finish_reason ? {
                        finish_reason: part.finish_reason
                    } : {},
                    ...part.logprobs ? {
                        logprobs: part.logprobs
                    } : {}
                };
                if ((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$ai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isAIMessage"])(generation.message)) {
                    generation.message.usage_metadata = usageMetadata;
                }
                // Fields are not serialized unless passed to the constructor
                // Doing this ensures all fields on the message are serialized
                generation.message = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$ai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["AIMessage"](Object.fromEntries(Object.entries(generation.message).filter((param)=>{
                    let [key] = param;
                    return !key.startsWith("lc_");
                })));
                generations.push(generation);
            }
            return {
                generations,
                llmOutput: {
                    tokenUsage: {
                        promptTokens: usageMetadata.input_tokens,
                        completionTokens: usageMetadata.output_tokens,
                        totalTokens: usageMetadata.total_tokens
                    }
                }
            };
        }
    }
    async *_streamResponseChunks(messages, options, runManager) {
        var _options_signal;
        const messagesMapped = _convertMessagesToOpenAIParams(messages, this.model);
        const params = {
            ...this.invocationParams(options, {
                streaming: true
            }),
            messages: messagesMapped,
            stream: true
        };
        let defaultRole;
        const streamIterable = await this.completionWithRetry(params, options);
        let usage;
        for await (const data of streamIterable){
            var _data_choices;
            const choice = data === null || data === void 0 ? void 0 : (_data_choices = data.choices) === null || _data_choices === void 0 ? void 0 : _data_choices[0];
            if (data.usage) {
                usage = data.usage;
            }
            if (!choice) {
                continue;
            }
            const { delta } = choice;
            if (!delta) {
                continue;
            }
            const chunk = this._convertCompletionsDeltaToBaseMessageChunk(delta, data, defaultRole);
            var _delta_role;
            defaultRole = (_delta_role = delta.role) !== null && _delta_role !== void 0 ? _delta_role : defaultRole;
            var _options_promptIndex, _choice_index;
            const newTokenIndices = {
                prompt: (_options_promptIndex = options.promptIndex) !== null && _options_promptIndex !== void 0 ? _options_promptIndex : 0,
                completion: (_choice_index = choice.index) !== null && _choice_index !== void 0 ? _choice_index : 0
            };
            if (typeof chunk.content !== "string") {
                console.log("[WARNING]: Received non-string content from OpenAI. This is currently not supported.");
                continue;
            }
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            const generationInfo = {
                ...newTokenIndices
            };
            if (choice.finish_reason != null) {
                generationInfo.finish_reason = choice.finish_reason;
                // Only include system fingerprint in the last chunk for now
                // to avoid concatenation issues
                generationInfo.system_fingerprint = data.system_fingerprint;
                generationInfo.model_name = data.model;
                generationInfo.service_tier = data.service_tier;
            }
            if (this.logprobs) {
                generationInfo.logprobs = choice.logprobs;
            }
            const generationChunk = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$outputs$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["ChatGenerationChunk"]({
                message: chunk,
                text: chunk.content,
                generationInfo
            });
            yield generationChunk;
            var _generationChunk_text;
            await (runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMNewToken((_generationChunk_text = generationChunk.text) !== null && _generationChunk_text !== void 0 ? _generationChunk_text : "", newTokenIndices, undefined, undefined, undefined, {
                chunk: generationChunk
            }));
        }
        if (usage) {
            var _usage_prompt_tokens_details, _usage_prompt_tokens_details1, _usage_prompt_tokens_details2, _usage_prompt_tokens_details3, _usage_completion_tokens_details, _usage_completion_tokens_details1, _usage_completion_tokens_details2, _usage_completion_tokens_details3;
            const inputTokenDetails = {
                ...((_usage_prompt_tokens_details = usage.prompt_tokens_details) === null || _usage_prompt_tokens_details === void 0 ? void 0 : _usage_prompt_tokens_details.audio_tokens) !== null && {
                    audio: (_usage_prompt_tokens_details1 = usage.prompt_tokens_details) === null || _usage_prompt_tokens_details1 === void 0 ? void 0 : _usage_prompt_tokens_details1.audio_tokens
                },
                ...((_usage_prompt_tokens_details2 = usage.prompt_tokens_details) === null || _usage_prompt_tokens_details2 === void 0 ? void 0 : _usage_prompt_tokens_details2.cached_tokens) !== null && {
                    cache_read: (_usage_prompt_tokens_details3 = usage.prompt_tokens_details) === null || _usage_prompt_tokens_details3 === void 0 ? void 0 : _usage_prompt_tokens_details3.cached_tokens
                }
            };
            const outputTokenDetails = {
                ...((_usage_completion_tokens_details = usage.completion_tokens_details) === null || _usage_completion_tokens_details === void 0 ? void 0 : _usage_completion_tokens_details.audio_tokens) !== null && {
                    audio: (_usage_completion_tokens_details1 = usage.completion_tokens_details) === null || _usage_completion_tokens_details1 === void 0 ? void 0 : _usage_completion_tokens_details1.audio_tokens
                },
                ...((_usage_completion_tokens_details2 = usage.completion_tokens_details) === null || _usage_completion_tokens_details2 === void 0 ? void 0 : _usage_completion_tokens_details2.reasoning_tokens) !== null && {
                    reasoning: (_usage_completion_tokens_details3 = usage.completion_tokens_details) === null || _usage_completion_tokens_details3 === void 0 ? void 0 : _usage_completion_tokens_details3.reasoning_tokens
                }
            };
            const generationChunk = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$outputs$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["ChatGenerationChunk"]({
                message: new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$ai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["AIMessageChunk"]({
                    content: "",
                    response_metadata: {
                        usage: {
                            ...usage
                        }
                    },
                    usage_metadata: {
                        input_tokens: usage.prompt_tokens,
                        output_tokens: usage.completion_tokens,
                        total_tokens: usage.total_tokens,
                        ...Object.keys(inputTokenDetails).length > 0 && {
                            input_token_details: inputTokenDetails
                        },
                        ...Object.keys(outputTokenDetails).length > 0 && {
                            output_token_details: outputTokenDetails
                        }
                    }
                }),
                text: ""
            });
            yield generationChunk;
        }
        if ((_options_signal = options.signal) === null || _options_signal === void 0 ? void 0 : _options_signal.aborted) {
            throw new Error("AbortError");
        }
    }
    async completionWithRetry(request, requestOptions) {
        const clientOptions = this._getClientOptions(requestOptions);
        const isParseableFormat = request.response_format && request.response_format.type === "json_schema";
        return this.caller.call(async ()=>{
            try {
                if (isParseableFormat && !request.stream) {
                    return await this.client.chat.completions.parse(request, clientOptions);
                } else {
                    return await this.client.chat.completions.create(request, clientOptions);
                }
            } catch (e) {
                const error = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__["wrapOpenAIClientError"])(e);
                throw error;
            }
        });
    }
    /** @internal */ _convertCompletionsMessageToBaseMessage(message, rawResponse) {
        const rawToolCalls = message.tool_calls;
        switch(message.role){
            case "assistant":
                {
                    var _rawResponse_choices_, _rawResponse_choices;
                    const toolCalls = [];
                    const invalidToolCalls = [];
                    for (const rawToolCall of rawToolCalls !== null && rawToolCalls !== void 0 ? rawToolCalls : []){
                        try {
                            toolCalls.push((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$openai_tools$2f$json_output_tools_parsers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["parseToolCall"])(rawToolCall, {
                                returnId: true
                            }));
                        // eslint-disable-next-line @typescript-eslint/no-explicit-any
                        } catch (e) {
                            invalidToolCalls.push((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$output_parsers$2f$openai_tools$2f$json_output_tools_parsers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["makeInvalidToolCall"])(rawToolCall, e.message));
                        }
                    }
                    const additional_kwargs = {
                        function_call: message.function_call,
                        tool_calls: rawToolCalls
                    };
                    if (this.__includeRawResponse !== undefined) {
                        additional_kwargs.__raw_response = rawResponse;
                    }
                    const response_metadata = {
                        model_name: rawResponse.model,
                        ...rawResponse.system_fingerprint ? {
                            usage: {
                                ...rawResponse.usage
                            },
                            system_fingerprint: rawResponse.system_fingerprint
                        } : {}
                    };
                    if (message.audio) {
                        additional_kwargs.audio = message.audio;
                    }
                    const content = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$output$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["handleMultiModalOutput"])(message.content || "", (_rawResponse_choices = rawResponse.choices) === null || _rawResponse_choices === void 0 ? void 0 : (_rawResponse_choices_ = _rawResponse_choices[0]) === null || _rawResponse_choices_ === void 0 ? void 0 : _rawResponse_choices_.message);
                    return new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$ai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["AIMessage"]({
                        content,
                        tool_calls: toolCalls,
                        invalid_tool_calls: invalidToolCalls,
                        additional_kwargs,
                        response_metadata,
                        id: rawResponse.id
                    });
                }
            default:
                var _message_role;
                return new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$chat$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["ChatMessage"](message.content || "", (_message_role = message.role) !== null && _message_role !== void 0 ? _message_role : "unknown");
        }
    }
    /** @internal */ _convertCompletionsDeltaToBaseMessageChunk(// eslint-disable-next-line @typescript-eslint/no-explicit-any
    delta, rawResponse, defaultRole) {
        var _delta_role;
        const role = (_delta_role = delta.role) !== null && _delta_role !== void 0 ? _delta_role : defaultRole;
        var _delta_content;
        const content = (_delta_content = delta.content) !== null && _delta_content !== void 0 ? _delta_content : "";
        let additional_kwargs;
        if (delta.function_call) {
            additional_kwargs = {
                function_call: delta.function_call
            };
        } else if (delta.tool_calls) {
            additional_kwargs = {
                tool_calls: delta.tool_calls
            };
        } else {
            additional_kwargs = {};
        }
        if (this.__includeRawResponse) {
            additional_kwargs.__raw_response = rawResponse;
        }
        if (delta.audio) {
            additional_kwargs.audio = {
                ...delta.audio,
                index: rawResponse.choices[0].index
            };
        }
        const response_metadata = {
            usage: {
                ...rawResponse.usage
            }
        };
        if (role === "user") {
            return new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$human$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["HumanMessageChunk"]({
                content,
                response_metadata
            });
        } else if (role === "assistant") {
            const toolCallChunks = [];
            if (Array.isArray(delta.tool_calls)) {
                for (const rawToolCall of delta.tool_calls){
                    var _rawToolCall_function, _rawToolCall_function1;
                    toolCallChunks.push({
                        name: (_rawToolCall_function = rawToolCall.function) === null || _rawToolCall_function === void 0 ? void 0 : _rawToolCall_function.name,
                        args: (_rawToolCall_function1 = rawToolCall.function) === null || _rawToolCall_function1 === void 0 ? void 0 : _rawToolCall_function1.arguments,
                        id: rawToolCall.id,
                        index: rawToolCall.index,
                        type: "tool_call_chunk"
                    });
                }
            }
            return new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$ai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["AIMessageChunk"]({
                content,
                tool_call_chunks: toolCallChunks,
                additional_kwargs,
                id: rawResponse.id,
                response_metadata
            });
        } else if (role === "system") {
            return new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$system$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["SystemMessageChunk"]({
                content,
                response_metadata
            });
        } else if (role === "developer") {
            return new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$system$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["SystemMessageChunk"]({
                content,
                response_metadata,
                additional_kwargs: {
                    __openai_role__: "developer"
                }
            });
        } else if (role === "function") {
            return new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$function$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["FunctionMessageChunk"]({
                content,
                additional_kwargs,
                name: delta.name,
                response_metadata
            });
        } else if (role === "tool") {
            return new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["ToolMessageChunk"]({
                content,
                additional_kwargs,
                tool_call_id: delta.tool_call_id,
                response_metadata
            });
        } else {
            return new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$messages$2f$chat$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["ChatMessageChunk"]({
                content,
                role,
                response_metadata
            });
        }
    }
}
class ChatOpenAI extends BaseChatOpenAI {
    get lc_serializable_keys() {
        return [
            ...super.lc_serializable_keys,
            "useResponsesApi"
        ];
    }
    _useResponsesApi(options) {
        var _options_tools, _options_reasoning, _this_reasoning, _options_tools1;
        const usesBuiltInTools = options === null || options === void 0 ? void 0 : (_options_tools = options.tools) === null || _options_tools === void 0 ? void 0 : _options_tools.some(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isBuiltInTool"]);
        const hasResponsesOnlyKwargs = (options === null || options === void 0 ? void 0 : options.previous_response_id) != null || (options === null || options === void 0 ? void 0 : options.text) != null || (options === null || options === void 0 ? void 0 : options.truncation) != null || (options === null || options === void 0 ? void 0 : options.include) != null || (options === null || options === void 0 ? void 0 : (_options_reasoning = options.reasoning) === null || _options_reasoning === void 0 ? void 0 : _options_reasoning.summary) != null || ((_this_reasoning = this.reasoning) === null || _this_reasoning === void 0 ? void 0 : _this_reasoning.summary) != null;
        const hasCustomTools = options === null || options === void 0 ? void 0 : (_options_tools1 = options.tools) === null || _options_tools1 === void 0 ? void 0 : _options_tools1.some(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["isOpenAICustomTool"]);
        return this.useResponsesApi || usesBuiltInTools || hasResponsesOnlyKwargs || hasCustomTools;
    }
    getLsParams(options) {
        const optionsWithDefaults = this._combineCallOptions(options);
        if (this._useResponsesApi(options)) {
            return this.responses.getLsParams(optionsWithDefaults);
        }
        return this.completions.getLsParams(optionsWithDefaults);
    }
    invocationParams(options) {
        const optionsWithDefaults = this._combineCallOptions(options);
        if (this._useResponsesApi(options)) {
            return this.responses.invocationParams(optionsWithDefaults);
        }
        return this.completions.invocationParams(optionsWithDefaults);
    }
    /** @ignore */ async _generate(messages, options, runManager) {
        if (this._useResponsesApi(options)) {
            return this.responses._generate(messages, options);
        }
        return this.completions._generate(messages, options, runManager);
    }
    async *_streamResponseChunks(messages, options, runManager) {
        if (this._useResponsesApi(options)) {
            yield* this.responses._streamResponseChunks(messages, this._combineCallOptions(options), runManager);
            return;
        }
        yield* this.completions._streamResponseChunks(messages, this._combineCallOptions(options), runManager);
    }
    withConfig(config) {
        const newModel = new ChatOpenAI(this.fields);
        newModel.defaultOptions = {
            ...this.defaultOptions,
            ...config
        };
        return newModel;
    }
    constructor(fields){
        super(fields);
        Object.defineProperty(this, "fields", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: fields
        });
        /**
         * Whether to use the responses API for all requests. If `false` the responses API will be used
         * only when required in order to fulfill the request.
         */ Object.defineProperty(this, "useResponsesApi", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "responses", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "completions", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        var _fields_useResponsesApi;
        this.useResponsesApi = (_fields_useResponsesApi = fields === null || fields === void 0 ? void 0 : fields.useResponsesApi) !== null && _fields_useResponsesApi !== void 0 ? _fields_useResponsesApi : false;
        var _fields_responses;
        this.responses = (_fields_responses = fields === null || fields === void 0 ? void 0 : fields.responses) !== null && _fields_responses !== void 0 ? _fields_responses : new ChatOpenAIResponses(fields);
        var _fields_completions;
        this.completions = (_fields_completions = fields === null || fields === void 0 ? void 0 : fields.completions) !== null && _fields_completions !== void 0 ? _fields_completions : new ChatOpenAICompletions(fields);
    }
}
}),
"[project]/node_modules/@langchain/openai/dist/utils/headers.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([
    "isHeaders",
    ()=>isHeaders,
    "normalizeHeaders",
    ()=>normalizeHeaders
]);
const iife = (fn)=>fn();
function isHeaders(headers) {
    return typeof Headers !== "undefined" && headers !== null && typeof headers === "object" && Object.prototype.toString.call(headers) === "[object Headers]";
}
function normalizeHeaders(headers) {
    const output = iife(()=>{
        // If headers is a Headers instance
        if (isHeaders(headers)) {
            return headers;
        } else if (Array.isArray(headers)) {
            return new Headers(headers);
        } else if (typeof headers === "object" && headers !== null && "values" in headers && isHeaders(headers.values)) {
            return headers.values;
        } else if (typeof headers === "object" && headers !== null) {
            const entries = Object.entries(headers).filter((param)=>{
                let [, v] = param;
                return typeof v === "string";
            }).map((param)=>{
                let [k, v] = param;
                return [
                    k,
                    v
                ];
            });
            return new Headers(entries);
        }
        return new Headers();
    });
    return Object.fromEntries(output.entries());
}
}),
"[project]/node_modules/@langchain/openai/dist/azure/chat_models.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([
    "AzureChatOpenAI",
    ()=>AzureChatOpenAI
]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$index$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/openai/index.mjs [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$azure$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/openai/azure.mjs [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/env.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/env.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$chat_models$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/chat_models.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$azure$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/azure.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$headers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/headers.js [app-client] (ecmascript)");
;
;
;
;
;
const AZURE_ALIASES = {
    openAIApiKey: "openai_api_key",
    openAIApiVersion: "openai_api_version",
    openAIBasePath: "openai_api_base",
    deploymentName: "deployment_name",
    azureOpenAIEndpoint: "azure_endpoint",
    azureOpenAIApiVersion: "openai_api_version",
    azureOpenAIBasePath: "openai_api_base",
    azureOpenAIApiDeploymentName: "deployment_name"
};
const AZURE_SECRETS = {
    azureOpenAIApiKey: "AZURE_OPENAI_API_KEY"
};
const AZURE_SERIALIZABLE_KEYS = [
    "azureOpenAIApiKey",
    "azureOpenAIApiVersion",
    "azureOpenAIBasePath",
    "azureOpenAIEndpoint",
    "azureOpenAIApiInstanceName",
    "azureOpenAIApiDeploymentName",
    "deploymentName",
    "openAIApiKey",
    "openAIApiVersion"
];
function _constructAzureFields(fields) {
    var _fields_azureOpenAIApiKey, _ref, _ref1;
    this.azureOpenAIApiKey = (_ref1 = (_ref = (_fields_azureOpenAIApiKey = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiKey) !== null && _fields_azureOpenAIApiKey !== void 0 ? _fields_azureOpenAIApiKey : fields === null || fields === void 0 ? void 0 : fields.openAIApiKey) !== null && _ref !== void 0 ? _ref : fields === null || fields === void 0 ? void 0 : fields.apiKey) !== null && _ref1 !== void 0 ? _ref1 : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_KEY");
    var _fields_azureOpenAIApiInstanceName;
    this.azureOpenAIApiInstanceName = (_fields_azureOpenAIApiInstanceName = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiInstanceName) !== null && _fields_azureOpenAIApiInstanceName !== void 0 ? _fields_azureOpenAIApiInstanceName : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_INSTANCE_NAME");
    var _fields_azureOpenAIApiDeploymentName, _ref2;
    this.azureOpenAIApiDeploymentName = (_ref2 = (_fields_azureOpenAIApiDeploymentName = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiDeploymentName) !== null && _fields_azureOpenAIApiDeploymentName !== void 0 ? _fields_azureOpenAIApiDeploymentName : fields === null || fields === void 0 ? void 0 : fields.deploymentName) !== null && _ref2 !== void 0 ? _ref2 : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_DEPLOYMENT_NAME");
    var _fields_azureOpenAIApiVersion, _ref3;
    this.azureOpenAIApiVersion = (_ref3 = (_fields_azureOpenAIApiVersion = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiVersion) !== null && _fields_azureOpenAIApiVersion !== void 0 ? _fields_azureOpenAIApiVersion : fields === null || fields === void 0 ? void 0 : fields.openAIApiVersion) !== null && _ref3 !== void 0 ? _ref3 : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_VERSION");
    var _fields_azureOpenAIBasePath;
    this.azureOpenAIBasePath = (_fields_azureOpenAIBasePath = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIBasePath) !== null && _fields_azureOpenAIBasePath !== void 0 ? _fields_azureOpenAIBasePath : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_BASE_PATH");
    var _fields_azureOpenAIEndpoint;
    this.azureOpenAIEndpoint = (_fields_azureOpenAIEndpoint = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIEndpoint) !== null && _fields_azureOpenAIEndpoint !== void 0 ? _fields_azureOpenAIEndpoint : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_ENDPOINT");
    this.azureADTokenProvider = fields === null || fields === void 0 ? void 0 : fields.azureADTokenProvider;
    if (!this.azureOpenAIApiKey && !this.apiKey && !this.azureADTokenProvider) {
        throw new Error("Azure OpenAI API key or Token Provider not found");
    }
}
function _getAzureClientOptions(options) {
    if (!this.client) {
        const openAIEndpointConfig = {
            azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,
            azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,
            azureOpenAIApiKey: this.azureOpenAIApiKey,
            azureOpenAIBasePath: this.azureOpenAIBasePath,
            azureADTokenProvider: this.azureADTokenProvider,
            baseURL: this.clientConfig.baseURL,
            azureOpenAIEndpoint: this.azureOpenAIEndpoint
        };
        const endpoint = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$azure$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEndpoint"])(openAIEndpointConfig);
        const params = {
            ...this.clientConfig,
            baseURL: endpoint,
            timeout: this.timeout,
            maxRetries: 0
        };
        if (!this.azureADTokenProvider) {
            params.apiKey = openAIEndpointConfig.azureOpenAIApiKey;
        }
        if (!params.baseURL) {
            delete params.baseURL;
        }
        let env = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnv"])();
        if (env === "node" || env === "deno") {
            env = "(".concat(env, "/").concat(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].version, "; ").concat(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].platform, "; ").concat(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].arch, ")");
        }
        const defaultHeaders = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$headers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["normalizeHeaders"])(params.defaultHeaders);
        params.defaultHeaders = {
            ...params.defaultHeaders,
            "User-Agent": defaultHeaders["User-Agent"] ? "langchainjs-azure-openai/2.0.0 (".concat(env, ")").concat(defaultHeaders["User-Agent"]) : "langchainjs-azure-openai/2.0.0 (".concat(env, ")")
        };
        this.client = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$azure$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__["AzureOpenAI"]({
            apiVersion: this.azureOpenAIApiVersion,
            azureADTokenProvider: this.azureADTokenProvider,
            deployment: this.azureOpenAIApiDeploymentName,
            ...params
        });
    }
    const requestOptions = {
        ...this.clientConfig,
        ...options
    };
    if (this.azureOpenAIApiKey) {
        requestOptions.headers = {
            "api-key": this.azureOpenAIApiKey,
            ...requestOptions.headers
        };
        requestOptions.query = {
            "api-version": this.azureOpenAIApiVersion,
            ...requestOptions.query
        };
    }
    return requestOptions;
}
function _serializeAzureChat(input) {
    const json = input;
    function isRecord(obj) {
        return typeof obj === "object" && obj != null;
    }
    if (isRecord(json) && isRecord(json.kwargs)) {
        delete json.kwargs.azure_openai_base_path;
        delete json.kwargs.azure_openai_api_deployment_name;
        delete json.kwargs.azure_openai_api_key;
        delete json.kwargs.azure_openai_api_version;
        delete json.kwargs.azure_open_ai_base_path;
        if (!json.kwargs.azure_endpoint && this.azureOpenAIEndpoint) {
            json.kwargs.azure_endpoint = this.azureOpenAIEndpoint;
        }
        if (!json.kwargs.azure_endpoint && this.azureOpenAIBasePath) {
            const parts = this.azureOpenAIBasePath.split("/openai/deployments/");
            if (parts.length === 2 && parts[0].startsWith("http")) {
                const [endpoint] = parts;
                json.kwargs.azure_endpoint = endpoint;
            }
        }
        if (!json.kwargs.azure_endpoint && this.azureOpenAIApiInstanceName) {
            json.kwargs.azure_endpoint = "https://".concat(this.azureOpenAIApiInstanceName, ".openai.azure.com/");
        }
        if (!json.kwargs.deployment_name && this.azureOpenAIApiDeploymentName) {
            json.kwargs.deployment_name = this.azureOpenAIApiDeploymentName;
        }
        if (!json.kwargs.deployment_name && this.azureOpenAIBasePath) {
            const parts = this.azureOpenAIBasePath.split("/openai/deployments/");
            if (parts.length === 2) {
                const [, deployment] = parts;
                json.kwargs.deployment_name = deployment;
            }
        }
        if (json.kwargs.azure_endpoint && json.kwargs.deployment_name && json.kwargs.openai_api_base) {
            delete json.kwargs.openai_api_base;
        }
        if (json.kwargs.azure_openai_api_instance_name && json.kwargs.azure_endpoint) {
            delete json.kwargs.azure_openai_api_instance_name;
        }
    }
    return json;
}
class AzureChatOpenAIResponses extends __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$chat_models$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["ChatOpenAIResponses"] {
    _llmType() {
        return "azure_openai";
    }
    get lc_aliases() {
        return {
            ...super.lc_aliases,
            ...AZURE_ALIASES
        };
    }
    get lc_secrets() {
        return {
            ...super.lc_secrets,
            ...AZURE_SECRETS
        };
    }
    get lc_serializable_keys() {
        return [
            ...super.lc_serializable_keys,
            ...AZURE_SERIALIZABLE_KEYS
        ];
    }
    getLsParams(options) {
        const params = super.getLsParams(options);
        params.ls_provider = "azure";
        return params;
    }
    _getClientOptions(options) {
        return _getAzureClientOptions.call(this, options);
    }
    toJSON() {
        return _serializeAzureChat.call(this, super.toJSON());
    }
    constructor(fields){
        super(fields);
        Object.defineProperty(this, "azureOpenAIApiVersion", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiKey", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureADTokenProvider", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiInstanceName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiDeploymentName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIBasePath", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIEndpoint", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        _constructAzureFields.call(this, fields);
    }
}
class AzureChatOpenAICompletions extends __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$chat_models$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["ChatOpenAICompletions"] {
    _llmType() {
        return "azure_openai";
    }
    get lc_aliases() {
        return {
            ...super.lc_aliases,
            ...AZURE_ALIASES
        };
    }
    get lc_secrets() {
        return {
            ...super.lc_secrets,
            ...AZURE_SECRETS
        };
    }
    get lc_serializable_keys() {
        return [
            ...super.lc_serializable_keys,
            ...AZURE_SERIALIZABLE_KEYS
        ];
    }
    getLsParams(options) {
        const params = super.getLsParams(options);
        params.ls_provider = "azure";
        return params;
    }
    _getClientOptions(options) {
        return _getAzureClientOptions.call(this, options);
    }
    toJSON() {
        return _serializeAzureChat.call(this, super.toJSON());
    }
    constructor(fields){
        super(fields);
        Object.defineProperty(this, "azureOpenAIApiVersion", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiKey", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureADTokenProvider", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiInstanceName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiDeploymentName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIBasePath", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIEndpoint", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        _constructAzureFields.call(this, fields);
    }
}
class AzureChatOpenAI extends __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$chat_models$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["ChatOpenAI"] {
    _llmType() {
        return "azure_openai";
    }
    get lc_aliases() {
        return {
            ...super.lc_aliases,
            ...AZURE_ALIASES
        };
    }
    get lc_secrets() {
        return {
            ...super.lc_secrets,
            ...AZURE_SECRETS
        };
    }
    get lc_serializable_keys() {
        return [
            ...super.lc_serializable_keys,
            ...AZURE_SERIALIZABLE_KEYS
        ];
    }
    getLsParams(options) {
        const params = super.getLsParams(options);
        params.ls_provider = "azure";
        return params;
    }
    /** @internal */ _getStructuredOutputMethod(config) {
        const ensuredConfig = {
            ...config
        };
        // Not all Azure gpt-4o deployments models support jsonSchema yet
        if (this.model.startsWith("gpt-4o")) {
            if ((ensuredConfig === null || ensuredConfig === void 0 ? void 0 : ensuredConfig.method) === undefined) {
                return "functionCalling";
            }
        }
        return super._getStructuredOutputMethod(ensuredConfig);
    }
    toJSON() {
        return _serializeAzureChat.call(this, super.toJSON());
    }
    constructor(fields){
        super({
            ...fields,
            completions: new AzureChatOpenAICompletions(fields),
            responses: new AzureChatOpenAIResponses(fields)
        });
        Object.defineProperty(this, "azureOpenAIApiVersion", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiKey", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureADTokenProvider", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiInstanceName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiDeploymentName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIBasePath", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIEndpoint", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        _constructAzureFields.call(this, fields);
    }
}
}),
"[project]/node_modules/@langchain/openai/dist/llms.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([
    "OpenAI",
    ()=>OpenAI
]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$index$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/openai/index.mjs [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$client$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/openai/client.mjs [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$language_models$2f$base$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/language_models/base.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$language_models$2f$base$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/language_models/base.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$outputs$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/outputs.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$outputs$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/outputs.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/env.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/env.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$language_models$2f$llms$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/language_models/llms.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$language_models$2f$llms$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/language_models/llms.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$chunk_array$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/chunk_array.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$chunk_array$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/chunk_array.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$azure$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/azure.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/openai.js [app-client] (ecmascript) <locals>");
;
;
;
;
;
;
;
;
class OpenAI extends __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$language_models$2f$llms$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["BaseLLM"] {
    static lc_name() {
        return "OpenAI";
    }
    get callKeys() {
        return [
            ...super.callKeys,
            "options"
        ];
    }
    get lc_secrets() {
        return {
            openAIApiKey: "OPENAI_API_KEY",
            apiKey: "OPENAI_API_KEY",
            organization: "OPENAI_ORGANIZATION"
        };
    }
    get lc_aliases() {
        return {
            modelName: "model",
            openAIApiKey: "openai_api_key",
            apiKey: "openai_api_key"
        };
    }
    /**
     * Get the parameters used to invoke the model
     */ invocationParams(options) {
        var _options_stop;
        return {
            model: this.model,
            temperature: this.temperature,
            max_tokens: this.maxTokens,
            top_p: this.topP,
            frequency_penalty: this.frequencyPenalty,
            presence_penalty: this.presencePenalty,
            n: this.n,
            best_of: this.bestOf,
            logit_bias: this.logitBias,
            stop: (_options_stop = options === null || options === void 0 ? void 0 : options.stop) !== null && _options_stop !== void 0 ? _options_stop : this.stopSequences,
            user: this.user,
            stream: this.streaming,
            ...this.modelKwargs
        };
    }
    /** @ignore */ _identifyingParams() {
        return {
            model_name: this.model,
            ...this.invocationParams(),
            ...this.clientConfig
        };
    }
    /**
     * Get the identifying parameters for the model
     */ identifyingParams() {
        return this._identifyingParams();
    }
    /**
     * Call out to OpenAI's endpoint with k unique prompts
     *
     * @param [prompts] - The prompts to pass into the model.
     * @param [options] - Optional list of stop words to use when generating.
     * @param [runManager] - Optional callback manager to use when generating.
     *
     * @returns The full LLM output.
     *
     * @example
     * ```ts
     * import { OpenAI } from "langchain/llms/openai";
     * const openai = new OpenAI();
     * const response = await openai.generate(["Tell me a joke."]);
     * ```
     */ async _generate(prompts, options, runManager) {
        const subPrompts = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$chunk_array$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["chunkArray"])(prompts, this.batchSize);
        const choices = [];
        const tokenUsage = {};
        const params = this.invocationParams(options);
        if (params.max_tokens === -1) {
            if (prompts.length !== 1) {
                throw new Error("max_tokens set to -1 not supported for multiple inputs");
            }
            params.max_tokens = await (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$language_models$2f$base$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["calculateMaxTokens"])({
                prompt: prompts[0],
                // Cast here to allow for other models that may not fit the union
                modelName: this.model
            });
        }
        for(let i = 0; i < subPrompts.length; i += 1){
            const data = params.stream ? await (async ()=>{
                var _options_signal;
                const choices = [];
                let response;
                const stream = await this.completionWithRetry({
                    ...params,
                    stream: true,
                    prompt: subPrompts[i]
                }, options);
                for await (const message of stream){
                    // on the first message set the response properties
                    if (!response) {
                        response = {
                            id: message.id,
                            object: message.object,
                            created: message.created,
                            model: message.model
                        };
                    }
                    // on all messages, update choice
                    for (const part of message.choices){
                        if (!choices[part.index]) {
                            choices[part.index] = part;
                        } else {
                            const choice = choices[part.index];
                            choice.text += part.text;
                            choice.finish_reason = part.finish_reason;
                            choice.logprobs = part.logprobs;
                        }
                        void (runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMNewToken(part.text, {
                            prompt: Math.floor(part.index / this.n),
                            completion: part.index % this.n
                        }));
                    }
                }
                if ((_options_signal = options.signal) === null || _options_signal === void 0 ? void 0 : _options_signal.aborted) {
                    throw new Error("AbortError");
                }
                return {
                    ...response,
                    choices
                };
            })() : await this.completionWithRetry({
                ...params,
                stream: false,
                prompt: subPrompts[i]
            }, {
                signal: options.signal,
                ...options.options
            });
            choices.push(...data.choices);
            const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens } = data.usage ? data.usage : {
                completion_tokens: undefined,
                prompt_tokens: undefined,
                total_tokens: undefined
            };
            if (completionTokens) {
                var _tokenUsage_completionTokens;
                tokenUsage.completionTokens = ((_tokenUsage_completionTokens = tokenUsage.completionTokens) !== null && _tokenUsage_completionTokens !== void 0 ? _tokenUsage_completionTokens : 0) + completionTokens;
            }
            if (promptTokens) {
                var _tokenUsage_promptTokens;
                tokenUsage.promptTokens = ((_tokenUsage_promptTokens = tokenUsage.promptTokens) !== null && _tokenUsage_promptTokens !== void 0 ? _tokenUsage_promptTokens : 0) + promptTokens;
            }
            if (totalTokens) {
                var _tokenUsage_totalTokens;
                tokenUsage.totalTokens = ((_tokenUsage_totalTokens = tokenUsage.totalTokens) !== null && _tokenUsage_totalTokens !== void 0 ? _tokenUsage_totalTokens : 0) + totalTokens;
            }
        }
        const generations = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$chunk_array$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["chunkArray"])(choices, this.n).map((promptChoices)=>promptChoices.map((choice)=>{
                var _choice_text;
                return {
                    text: (_choice_text = choice.text) !== null && _choice_text !== void 0 ? _choice_text : "",
                    generationInfo: {
                        finishReason: choice.finish_reason,
                        logprobs: choice.logprobs
                    }
                };
            }));
        return {
            generations,
            llmOutput: {
                tokenUsage
            }
        };
    }
    // TODO(jacoblee): Refactor with _generate(..., {stream: true}) implementation?
    async *_streamResponseChunks(input, options, runManager) {
        var _options_signal;
        const params = {
            ...this.invocationParams(options),
            prompt: input,
            stream: true
        };
        const stream = await this.completionWithRetry(params, options);
        for await (const data of stream){
            const choice = data === null || data === void 0 ? void 0 : data.choices[0];
            if (!choice) {
                continue;
            }
            const chunk = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$outputs$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["GenerationChunk"]({
                text: choice.text,
                generationInfo: {
                    finishReason: choice.finish_reason
                }
            });
            yield chunk;
            var _chunk_text;
            // eslint-disable-next-line no-void
            void (runManager === null || runManager === void 0 ? void 0 : runManager.handleLLMNewToken((_chunk_text = chunk.text) !== null && _chunk_text !== void 0 ? _chunk_text : ""));
        }
        if ((_options_signal = options.signal) === null || _options_signal === void 0 ? void 0 : _options_signal.aborted) {
            throw new Error("AbortError");
        }
    }
    async completionWithRetry(request, options) {
        const requestOptions = this._getClientOptions(options);
        return this.caller.call(async ()=>{
            try {
                const res = await this.client.completions.create(request, requestOptions);
                return res;
            } catch (e) {
                const error = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__["wrapOpenAIClientError"])(e);
                throw error;
            }
        });
    }
    /**
     * Calls the OpenAI API with retry logic in case of failures.
     * @param request The request to send to the OpenAI API.
     * @param options Optional configuration for the API call.
     * @returns The response from the OpenAI API.
     */ _getClientOptions(options) {
        if (!this.client) {
            const openAIEndpointConfig = {
                baseURL: this.clientConfig.baseURL
            };
            const endpoint = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$azure$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEndpoint"])(openAIEndpointConfig);
            const params = {
                ...this.clientConfig,
                baseURL: endpoint,
                timeout: this.timeout,
                maxRetries: 0
            };
            if (!params.baseURL) {
                delete params.baseURL;
            }
            this.client = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$client$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__["OpenAI"](params);
        }
        const requestOptions = {
            ...this.clientConfig,
            ...options
        };
        return requestOptions;
    }
    _llmType() {
        return "openai";
    }
    constructor(fields){
        var _fields_configuration, _this_model, _this_model1, _this_model2, _this_model3;
        super(fields !== null && fields !== void 0 ? fields : {});
        Object.defineProperty(this, "lc_serializable", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: true
        });
        Object.defineProperty(this, "temperature", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "maxTokens", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "topP", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "frequencyPenalty", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "presencePenalty", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "n", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 1
        });
        Object.defineProperty(this, "bestOf", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "logitBias", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "model", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "gpt-3.5-turbo-instruct"
        });
        /** @deprecated Use "model" instead */ Object.defineProperty(this, "modelName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "modelKwargs", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "batchSize", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 20
        });
        Object.defineProperty(this, "timeout", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "stop", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "stopSequences", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "user", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "streaming", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "openAIApiKey", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "apiKey", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "organization", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "client", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "clientConfig", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        var _fields_apiKey, _ref;
        this.openAIApiKey = (_ref = (_fields_apiKey = fields === null || fields === void 0 ? void 0 : fields.apiKey) !== null && _fields_apiKey !== void 0 ? _fields_apiKey : fields === null || fields === void 0 ? void 0 : fields.openAIApiKey) !== null && _ref !== void 0 ? _ref : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("OPENAI_API_KEY");
        this.apiKey = this.openAIApiKey;
        var _fields_configuration_organization;
        this.organization = (_fields_configuration_organization = fields === null || fields === void 0 ? void 0 : (_fields_configuration = fields.configuration) === null || _fields_configuration === void 0 ? void 0 : _fields_configuration.organization) !== null && _fields_configuration_organization !== void 0 ? _fields_configuration_organization : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("OPENAI_ORGANIZATION");
        var _fields_model, _ref1;
        this.model = (_ref1 = (_fields_model = fields === null || fields === void 0 ? void 0 : fields.model) !== null && _fields_model !== void 0 ? _fields_model : fields === null || fields === void 0 ? void 0 : fields.modelName) !== null && _ref1 !== void 0 ? _ref1 : this.model;
        if ((((_this_model = this.model) === null || _this_model === void 0 ? void 0 : _this_model.startsWith("gpt-3.5-turbo")) || ((_this_model1 = this.model) === null || _this_model1 === void 0 ? void 0 : _this_model1.startsWith("gpt-4")) || ((_this_model2 = this.model) === null || _this_model2 === void 0 ? void 0 : _this_model2.startsWith("o1"))) && !((_this_model3 = this.model) === null || _this_model3 === void 0 ? void 0 : _this_model3.includes("-instruct"))) {
            throw new Error([
                'Your chosen OpenAI model, "'.concat(this.model, '", is a chat model and not a text-in/text-out LLM.'),
                'Passing it into the "OpenAI" class is no longer supported.',
                'Please use the "ChatOpenAI" class instead.',
                "",
                "See this page for more information:",
                "|",
                "└> https://js.langchain.com/docs/integrations/chat/openai"
            ].join("\n"));
        }
        this.modelName = this.model;
        var _fields_modelKwargs;
        this.modelKwargs = (_fields_modelKwargs = fields === null || fields === void 0 ? void 0 : fields.modelKwargs) !== null && _fields_modelKwargs !== void 0 ? _fields_modelKwargs : {};
        var _fields_batchSize;
        this.batchSize = (_fields_batchSize = fields === null || fields === void 0 ? void 0 : fields.batchSize) !== null && _fields_batchSize !== void 0 ? _fields_batchSize : this.batchSize;
        this.timeout = fields === null || fields === void 0 ? void 0 : fields.timeout;
        var _fields_temperature;
        this.temperature = (_fields_temperature = fields === null || fields === void 0 ? void 0 : fields.temperature) !== null && _fields_temperature !== void 0 ? _fields_temperature : this.temperature;
        var _fields_maxTokens;
        this.maxTokens = (_fields_maxTokens = fields === null || fields === void 0 ? void 0 : fields.maxTokens) !== null && _fields_maxTokens !== void 0 ? _fields_maxTokens : this.maxTokens;
        var _fields_topP;
        this.topP = (_fields_topP = fields === null || fields === void 0 ? void 0 : fields.topP) !== null && _fields_topP !== void 0 ? _fields_topP : this.topP;
        var _fields_frequencyPenalty;
        this.frequencyPenalty = (_fields_frequencyPenalty = fields === null || fields === void 0 ? void 0 : fields.frequencyPenalty) !== null && _fields_frequencyPenalty !== void 0 ? _fields_frequencyPenalty : this.frequencyPenalty;
        var _fields_presencePenalty;
        this.presencePenalty = (_fields_presencePenalty = fields === null || fields === void 0 ? void 0 : fields.presencePenalty) !== null && _fields_presencePenalty !== void 0 ? _fields_presencePenalty : this.presencePenalty;
        var _fields_n;
        this.n = (_fields_n = fields === null || fields === void 0 ? void 0 : fields.n) !== null && _fields_n !== void 0 ? _fields_n : this.n;
        var _fields_bestOf;
        this.bestOf = (_fields_bestOf = fields === null || fields === void 0 ? void 0 : fields.bestOf) !== null && _fields_bestOf !== void 0 ? _fields_bestOf : this.bestOf;
        this.logitBias = fields === null || fields === void 0 ? void 0 : fields.logitBias;
        var _fields_stopSequences;
        this.stop = (_fields_stopSequences = fields === null || fields === void 0 ? void 0 : fields.stopSequences) !== null && _fields_stopSequences !== void 0 ? _fields_stopSequences : fields === null || fields === void 0 ? void 0 : fields.stop;
        this.stopSequences = this.stop;
        this.user = fields === null || fields === void 0 ? void 0 : fields.user;
        var _fields_streaming;
        this.streaming = (_fields_streaming = fields === null || fields === void 0 ? void 0 : fields.streaming) !== null && _fields_streaming !== void 0 ? _fields_streaming : false;
        if (this.streaming && this.bestOf && this.bestOf > 1) {
            throw new Error("Cannot stream results when bestOf > 1");
        }
        this.clientConfig = {
            apiKey: this.apiKey,
            organization: this.organization,
            dangerouslyAllowBrowser: true,
            ...fields === null || fields === void 0 ? void 0 : fields.configuration
        };
    }
}
}),
"[project]/node_modules/@langchain/openai/dist/azure/llms.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([
    "AzureOpenAI",
    ()=>AzureOpenAI
]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$index$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/openai/index.mjs [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$azure$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/openai/azure.mjs [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/env.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/env.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$llms$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/llms.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$azure$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/azure.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$headers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/headers.js [app-client] (ecmascript)");
;
;
;
;
;
class AzureOpenAI extends __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$llms$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["OpenAI"] {
    get lc_aliases() {
        return {
            ...super.lc_aliases,
            openAIApiKey: "openai_api_key",
            openAIApiVersion: "openai_api_version",
            openAIBasePath: "openai_api_base",
            deploymentName: "deployment_name",
            azureOpenAIEndpoint: "azure_endpoint",
            azureOpenAIApiVersion: "openai_api_version",
            azureOpenAIBasePath: "openai_api_base",
            azureOpenAIApiDeploymentName: "deployment_name"
        };
    }
    get lc_secrets() {
        return {
            ...super.lc_secrets,
            azureOpenAIApiKey: "AZURE_OPENAI_API_KEY"
        };
    }
    _getClientOptions(options) {
        if (!this.client) {
            const openAIEndpointConfig = {
                azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,
                azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,
                azureOpenAIApiKey: this.azureOpenAIApiKey,
                azureOpenAIBasePath: this.azureOpenAIBasePath,
                azureADTokenProvider: this.azureADTokenProvider,
                baseURL: this.clientConfig.baseURL
            };
            const endpoint = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$azure$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEndpoint"])(openAIEndpointConfig);
            const params = {
                ...this.clientConfig,
                baseURL: endpoint,
                timeout: this.timeout,
                maxRetries: 0
            };
            if (!this.azureADTokenProvider) {
                params.apiKey = openAIEndpointConfig.azureOpenAIApiKey;
            }
            if (!params.baseURL) {
                delete params.baseURL;
            }
            const defaultHeaders = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$headers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["normalizeHeaders"])(params.defaultHeaders);
            params.defaultHeaders = {
                ...params.defaultHeaders,
                "User-Agent": defaultHeaders["User-Agent"] ? "".concat(defaultHeaders["User-Agent"], ": langchainjs-azure-openai-v2") : "langchainjs-azure-openai-v2"
            };
            this.client = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$azure$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__["AzureOpenAI"]({
                apiVersion: this.azureOpenAIApiVersion,
                azureADTokenProvider: this.azureADTokenProvider,
                ...params
            });
        }
        const requestOptions = {
            ...this.clientConfig,
            ...options
        };
        if (this.azureOpenAIApiKey) {
            requestOptions.headers = {
                "api-key": this.azureOpenAIApiKey,
                ...requestOptions.headers
            };
            requestOptions.query = {
                "api-version": this.azureOpenAIApiVersion,
                ...requestOptions.query
            };
        }
        return requestOptions;
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    toJSON() {
        const json = super.toJSON();
        function isRecord(obj) {
            return typeof obj === "object" && obj != null;
        }
        if (isRecord(json) && isRecord(json.kwargs)) {
            delete json.kwargs.azure_openai_base_path;
            delete json.kwargs.azure_openai_api_deployment_name;
            delete json.kwargs.azure_openai_api_key;
            delete json.kwargs.azure_openai_api_version;
            delete json.kwargs.azure_open_ai_base_path;
        }
        return json;
    }
    constructor(fields){
        super(fields);
        Object.defineProperty(this, "azureOpenAIApiVersion", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiKey", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureADTokenProvider", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiInstanceName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiDeploymentName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIBasePath", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIEndpoint", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        var _ref;
        this.azureOpenAIApiDeploymentName = (_ref = (fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiCompletionsDeploymentName) || (fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiDeploymentName)) !== null && _ref !== void 0 ? _ref : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME") || (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_DEPLOYMENT_NAME");
        var _fields_azureOpenAIApiKey, _ref1, _ref2;
        this.azureOpenAIApiKey = (_ref2 = (_ref1 = (_fields_azureOpenAIApiKey = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiKey) !== null && _fields_azureOpenAIApiKey !== void 0 ? _fields_azureOpenAIApiKey : fields === null || fields === void 0 ? void 0 : fields.openAIApiKey) !== null && _ref1 !== void 0 ? _ref1 : fields === null || fields === void 0 ? void 0 : fields.apiKey) !== null && _ref2 !== void 0 ? _ref2 : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_KEY");
        var _fields_azureOpenAIApiInstanceName;
        this.azureOpenAIApiInstanceName = (_fields_azureOpenAIApiInstanceName = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiInstanceName) !== null && _fields_azureOpenAIApiInstanceName !== void 0 ? _fields_azureOpenAIApiInstanceName : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_INSTANCE_NAME");
        var _fields_azureOpenAIApiVersion, _ref3;
        this.azureOpenAIApiVersion = (_ref3 = (_fields_azureOpenAIApiVersion = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiVersion) !== null && _fields_azureOpenAIApiVersion !== void 0 ? _fields_azureOpenAIApiVersion : fields === null || fields === void 0 ? void 0 : fields.openAIApiVersion) !== null && _ref3 !== void 0 ? _ref3 : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_VERSION");
        var _fields_azureOpenAIBasePath;
        this.azureOpenAIBasePath = (_fields_azureOpenAIBasePath = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIBasePath) !== null && _fields_azureOpenAIBasePath !== void 0 ? _fields_azureOpenAIBasePath : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_BASE_PATH");
        var _fields_azureOpenAIEndpoint;
        this.azureOpenAIEndpoint = (_fields_azureOpenAIEndpoint = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIEndpoint) !== null && _fields_azureOpenAIEndpoint !== void 0 ? _fields_azureOpenAIEndpoint : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_ENDPOINT");
        this.azureADTokenProvider = fields === null || fields === void 0 ? void 0 : fields.azureADTokenProvider;
        if (!this.azureOpenAIApiKey && !this.apiKey && !this.azureADTokenProvider) {
            throw new Error("Azure OpenAI API key or Token Provider not found");
        }
    }
}
}),
"[project]/node_modules/@langchain/openai/dist/embeddings.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([
    "OpenAIEmbeddings",
    ()=>OpenAIEmbeddings
]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$index$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/openai/index.mjs [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$client$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/openai/client.mjs [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/env.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/env.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$embeddings$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/embeddings.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$embeddings$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/embeddings.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$chunk_array$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/chunk_array.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$chunk_array$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/chunk_array.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$azure$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/azure.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/openai.js [app-client] (ecmascript) <locals>");
;
;
;
;
;
;
class OpenAIEmbeddings extends __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$embeddings$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Embeddings"] {
    /**
     * Method to generate embeddings for an array of documents. Splits the
     * documents into batches and makes requests to the OpenAI API to generate
     * embeddings.
     * @param texts Array of documents to generate embeddings for.
     * @returns Promise that resolves to a 2D array of embeddings for each document.
     */ async embedDocuments(texts) {
        const batches = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$chunk_array$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["chunkArray"])(this.stripNewLines ? texts.map((t)=>t.replace(/\n/g, " ")) : texts, this.batchSize);
        const batchRequests = batches.map((batch)=>{
            const params = {
                model: this.model,
                input: batch
            };
            if (this.dimensions) {
                params.dimensions = this.dimensions;
            }
            if (this.encodingFormat) {
                params.encoding_format = this.encodingFormat;
            }
            return this.embeddingWithRetry(params);
        });
        const batchResponses = await Promise.all(batchRequests);
        const embeddings = [];
        for(let i = 0; i < batchResponses.length; i += 1){
            const batch = batches[i];
            const { data: batchResponse } = batchResponses[i];
            for(let j = 0; j < batch.length; j += 1){
                embeddings.push(batchResponse[j].embedding);
            }
        }
        return embeddings;
    }
    /**
     * Method to generate an embedding for a single document. Calls the
     * embeddingWithRetry method with the document as the input.
     * @param text Document to generate an embedding for.
     * @returns Promise that resolves to an embedding for the document.
     */ async embedQuery(text) {
        const params = {
            model: this.model,
            input: this.stripNewLines ? text.replace(/\n/g, " ") : text
        };
        if (this.dimensions) {
            params.dimensions = this.dimensions;
        }
        if (this.encodingFormat) {
            params.encoding_format = this.encodingFormat;
        }
        const { data } = await this.embeddingWithRetry(params);
        return data[0].embedding;
    }
    /**
     * Private method to make a request to the OpenAI API to generate
     * embeddings. Handles the retry logic and returns the response from the
     * API.
     * @param request Request to send to the OpenAI API.
     * @returns Promise that resolves to the response from the API.
     */ async embeddingWithRetry(request) {
        if (!this.client) {
            const openAIEndpointConfig = {
                baseURL: this.clientConfig.baseURL
            };
            const endpoint = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$azure$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEndpoint"])(openAIEndpointConfig);
            const params = {
                ...this.clientConfig,
                baseURL: endpoint,
                timeout: this.timeout,
                maxRetries: 0
            };
            if (!params.baseURL) {
                delete params.baseURL;
            }
            this.client = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$client$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__["OpenAI"](params);
        }
        const requestOptions = {};
        return this.caller.call(async ()=>{
            try {
                const res = await this.client.embeddings.create(request, requestOptions);
                return res;
            } catch (e) {
                const error = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__["wrapOpenAIClientError"])(e);
                throw error;
            }
        });
    }
    constructor(fields){
        var _fieldsWithDefaults_configuration;
        const fieldsWithDefaults = {
            maxConcurrency: 2,
            ...fields
        };
        super(fieldsWithDefaults);
        Object.defineProperty(this, "model", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "text-embedding-ada-002"
        });
        /** @deprecated Use "model" instead */ Object.defineProperty(this, "modelName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "batchSize", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 512
        });
        // TODO: Update to `false` on next minor release (see: https://github.com/langchain-ai/langchainjs/pull/3612)
        Object.defineProperty(this, "stripNewLines", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: true
        });
        /**
         * The number of dimensions the resulting output embeddings should have.
         * Only supported in `text-embedding-3` and later models.
         */ Object.defineProperty(this, "dimensions", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "timeout", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "organization", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "encodingFormat", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "client", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "clientConfig", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        var _fieldsWithDefaults_apiKey, _ref;
        const apiKey = (_ref = (_fieldsWithDefaults_apiKey = fieldsWithDefaults === null || fieldsWithDefaults === void 0 ? void 0 : fieldsWithDefaults.apiKey) !== null && _fieldsWithDefaults_apiKey !== void 0 ? _fieldsWithDefaults_apiKey : fieldsWithDefaults === null || fieldsWithDefaults === void 0 ? void 0 : fieldsWithDefaults.openAIApiKey) !== null && _ref !== void 0 ? _ref : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("OPENAI_API_KEY");
        var _fieldsWithDefaults_configuration_organization;
        this.organization = (_fieldsWithDefaults_configuration_organization = fieldsWithDefaults === null || fieldsWithDefaults === void 0 ? void 0 : (_fieldsWithDefaults_configuration = fieldsWithDefaults.configuration) === null || _fieldsWithDefaults_configuration === void 0 ? void 0 : _fieldsWithDefaults_configuration.organization) !== null && _fieldsWithDefaults_configuration_organization !== void 0 ? _fieldsWithDefaults_configuration_organization : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("OPENAI_ORGANIZATION");
        var _fieldsWithDefaults_model, _ref1;
        this.model = (_ref1 = (_fieldsWithDefaults_model = fieldsWithDefaults === null || fieldsWithDefaults === void 0 ? void 0 : fieldsWithDefaults.model) !== null && _fieldsWithDefaults_model !== void 0 ? _fieldsWithDefaults_model : fieldsWithDefaults === null || fieldsWithDefaults === void 0 ? void 0 : fieldsWithDefaults.modelName) !== null && _ref1 !== void 0 ? _ref1 : this.model;
        this.modelName = this.model;
        var _fieldsWithDefaults_batchSize;
        this.batchSize = (_fieldsWithDefaults_batchSize = fieldsWithDefaults === null || fieldsWithDefaults === void 0 ? void 0 : fieldsWithDefaults.batchSize) !== null && _fieldsWithDefaults_batchSize !== void 0 ? _fieldsWithDefaults_batchSize : this.batchSize;
        var _fieldsWithDefaults_stripNewLines;
        this.stripNewLines = (_fieldsWithDefaults_stripNewLines = fieldsWithDefaults === null || fieldsWithDefaults === void 0 ? void 0 : fieldsWithDefaults.stripNewLines) !== null && _fieldsWithDefaults_stripNewLines !== void 0 ? _fieldsWithDefaults_stripNewLines : this.stripNewLines;
        this.timeout = fieldsWithDefaults === null || fieldsWithDefaults === void 0 ? void 0 : fieldsWithDefaults.timeout;
        this.dimensions = fieldsWithDefaults === null || fieldsWithDefaults === void 0 ? void 0 : fieldsWithDefaults.dimensions;
        this.encodingFormat = fieldsWithDefaults === null || fieldsWithDefaults === void 0 ? void 0 : fieldsWithDefaults.encodingFormat;
        this.clientConfig = {
            apiKey,
            organization: this.organization,
            dangerouslyAllowBrowser: true,
            ...fields === null || fields === void 0 ? void 0 : fields.configuration
        };
    }
}
}),
"[project]/node_modules/@langchain/openai/dist/azure/embeddings.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([
    "AzureOpenAIEmbeddings",
    ()=>AzureOpenAIEmbeddings
]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$index$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/openai/index.mjs [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$azure$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/openai/azure.mjs [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/env.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/env.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$embeddings$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/embeddings.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$azure$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/azure.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/openai.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$headers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/headers.js [app-client] (ecmascript)");
;
;
;
;
;
;
class AzureOpenAIEmbeddings extends __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$embeddings$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["OpenAIEmbeddings"] {
    async embeddingWithRetry(request) {
        if (!this.client) {
            const openAIEndpointConfig = {
                azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,
                azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,
                azureOpenAIApiKey: this.azureOpenAIApiKey,
                azureOpenAIBasePath: this.azureOpenAIBasePath,
                azureADTokenProvider: this.azureADTokenProvider,
                baseURL: this.clientConfig.baseURL
            };
            const endpoint = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$azure$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEndpoint"])(openAIEndpointConfig);
            const params = {
                ...this.clientConfig,
                baseURL: endpoint,
                timeout: this.timeout,
                maxRetries: 0
            };
            if (!this.azureADTokenProvider) {
                params.apiKey = openAIEndpointConfig.azureOpenAIApiKey;
            }
            if (!params.baseURL) {
                delete params.baseURL;
            }
            const defaultHeaders = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$headers$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["normalizeHeaders"])(params.defaultHeaders);
            params.defaultHeaders = {
                ...params.defaultHeaders,
                "User-Agent": defaultHeaders["User-Agent"] ? "".concat(defaultHeaders["User-Agent"], ": langchainjs-azure-openai-v2") : "langchainjs-azure-openai-v2"
            };
            this.client = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$azure$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__["AzureOpenAI"]({
                apiVersion: this.azureOpenAIApiVersion,
                azureADTokenProvider: this.azureADTokenProvider,
                deployment: this.azureOpenAIApiDeploymentName,
                ...params
            });
        }
        const requestOptions = {};
        if (this.azureOpenAIApiKey) {
            requestOptions.headers = {
                "api-key": this.azureOpenAIApiKey,
                ...requestOptions.headers
            };
            requestOptions.query = {
                "api-version": this.azureOpenAIApiVersion,
                ...requestOptions.query
            };
        }
        return this.caller.call(async ()=>{
            try {
                const res = await this.client.embeddings.create(request, requestOptions);
                return res;
            } catch (e) {
                const error = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__["wrapOpenAIClientError"])(e);
                throw error;
            }
        });
    }
    constructor(fields){
        super(fields);
        Object.defineProperty(this, "azureOpenAIApiVersion", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiKey", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureADTokenProvider", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiInstanceName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIApiDeploymentName", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "azureOpenAIBasePath", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        var _fields_batchSize;
        this.batchSize = (_fields_batchSize = fields === null || fields === void 0 ? void 0 : fields.batchSize) !== null && _fields_batchSize !== void 0 ? _fields_batchSize : 1;
        var _fields_azureOpenAIApiKey, _ref;
        this.azureOpenAIApiKey = (_ref = (_fields_azureOpenAIApiKey = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiKey) !== null && _fields_azureOpenAIApiKey !== void 0 ? _fields_azureOpenAIApiKey : fields === null || fields === void 0 ? void 0 : fields.apiKey) !== null && _ref !== void 0 ? _ref : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_KEY");
        var _fields_azureOpenAIApiVersion, _ref1;
        this.azureOpenAIApiVersion = (_ref1 = (_fields_azureOpenAIApiVersion = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiVersion) !== null && _fields_azureOpenAIApiVersion !== void 0 ? _fields_azureOpenAIApiVersion : fields === null || fields === void 0 ? void 0 : fields.openAIApiVersion) !== null && _ref1 !== void 0 ? _ref1 : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_VERSION");
        var _fields_azureOpenAIBasePath;
        this.azureOpenAIBasePath = (_fields_azureOpenAIBasePath = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIBasePath) !== null && _fields_azureOpenAIBasePath !== void 0 ? _fields_azureOpenAIBasePath : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_BASE_PATH");
        var _fields_azureOpenAIApiInstanceName;
        this.azureOpenAIApiInstanceName = (_fields_azureOpenAIApiInstanceName = fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiInstanceName) !== null && _fields_azureOpenAIApiInstanceName !== void 0 ? _fields_azureOpenAIApiInstanceName : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_INSTANCE_NAME");
        var _ref2;
        this.azureOpenAIApiDeploymentName = (_ref2 = (fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiEmbeddingsDeploymentName) || (fields === null || fields === void 0 ? void 0 : fields.azureOpenAIApiDeploymentName)) !== null && _ref2 !== void 0 ? _ref2 : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME") || (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("AZURE_OPENAI_API_DEPLOYMENT_NAME");
        this.azureADTokenProvider = fields === null || fields === void 0 ? void 0 : fields.azureADTokenProvider;
    }
}
}),
"[project]/node_modules/@langchain/openai/dist/types.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([]);
;
}),
"[project]/node_modules/@langchain/openai/dist/tools/dalle.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

/* eslint-disable no-param-reassign */ __turbopack_context__.s([
    "DallEAPIWrapper",
    ()=>DallEAPIWrapper
]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/utils/env.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/utils/env.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$index$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/openai/index.mjs [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$client$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/openai/client.mjs [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/tools.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$tools$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/tools/index.js [app-client] (ecmascript)");
;
;
;
class DallEAPIWrapper extends __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$tools$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Tool"] {
    static lc_name() {
        return "DallEAPIWrapper";
    }
    /**
     * Processes the API response if multiple images are generated.
     * Returns a list of MessageContentImageUrl objects. If the response
     * format is `url`, then the `image_url` field will contain the URL.
     * If it is `b64_json`, then the `image_url` field will contain an object
     * with a `url` field with the base64 encoded image.
     *
     * @param {OpenAIClient.Images.ImagesResponse[]} response The API response
     * @returns {MessageContentImageUrl[]}
     */ processMultipleGeneratedUrls(response) {
        if (this.dallEResponseFormat === "url") {
            return response.flatMap((res)=>{
                var _res_data;
                var _res_data_flatMap_filter;
                const imageUrlContent = (_res_data_flatMap_filter = (_res_data = res.data) === null || _res_data === void 0 ? void 0 : _res_data.flatMap((item)=>{
                    if (!item.url) return [];
                    return {
                        type: "image_url",
                        image_url: item.url
                    };
                }).filter((item)=>item !== undefined && item.type === "image_url" && typeof item.image_url === "string" && item.image_url !== undefined)) !== null && _res_data_flatMap_filter !== void 0 ? _res_data_flatMap_filter : [];
                return imageUrlContent;
            });
        } else {
            return response.flatMap((res)=>{
                var _res_data;
                var _res_data_flatMap_filter;
                const b64Content = (_res_data_flatMap_filter = (_res_data = res.data) === null || _res_data === void 0 ? void 0 : _res_data.flatMap((item)=>{
                    if (!item.b64_json) return [];
                    return {
                        type: "image_url",
                        image_url: {
                            url: item.b64_json
                        }
                    };
                }).filter((item)=>item !== undefined && item.type === "image_url" && typeof item.image_url === "object" && "url" in item.image_url && typeof item.image_url.url === "string" && item.image_url.url !== undefined)) !== null && _res_data_flatMap_filter !== void 0 ? _res_data_flatMap_filter : [];
                return b64Content;
            });
        }
    }
    /** @ignore */ async _call(input) {
        const generateImageFields = {
            model: this.model,
            prompt: input,
            n: 1,
            size: this.size,
            response_format: this.dallEResponseFormat,
            style: this.style,
            quality: this.quality,
            user: this.user
        };
        if (this.n > 1) {
            const results = await Promise.all(Array.from({
                length: this.n
            }).map(()=>this.client.images.generate(generateImageFields)));
            return this.processMultipleGeneratedUrls(results);
        }
        const response = await this.client.images.generate(generateImageFields);
        let data = "";
        if (this.dallEResponseFormat === "url") {
            var _response_data;
            var _response_data_map_filter;
            [data] = (_response_data_map_filter = (_response_data = response.data) === null || _response_data === void 0 ? void 0 : _response_data.map((item)=>item.url).filter((url)=>url !== "undefined")) !== null && _response_data_map_filter !== void 0 ? _response_data_map_filter : [];
        } else {
            var _response_data1;
            var _response_data_map_filter1;
            [data] = (_response_data_map_filter1 = (_response_data1 = response.data) === null || _response_data1 === void 0 ? void 0 : _response_data1.map((item)=>item.b64_json).filter((b64_json)=>b64_json !== "undefined")) !== null && _response_data_map_filter1 !== void 0 ? _response_data_map_filter1 : [];
        }
        return data;
    }
    constructor(fields){
        // Shim for new base tool param name
        if ((fields === null || fields === void 0 ? void 0 : fields.responseFormat) !== undefined && [
            "url",
            "b64_json"
        ].includes(fields.responseFormat)) {
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            fields.dallEResponseFormat = fields.responseFormat;
            fields.responseFormat = "content";
        }
        super(fields);
        Object.defineProperty(this, "name", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "dalle_api_wrapper"
        });
        Object.defineProperty(this, "description", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "A wrapper around OpenAI DALL-E API. Useful for when you need to generate images from a text description. Input should be an image description."
        });
        Object.defineProperty(this, "client", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "model", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "dall-e-3"
        });
        Object.defineProperty(this, "style", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "vivid"
        });
        Object.defineProperty(this, "quality", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "standard"
        });
        Object.defineProperty(this, "n", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 1
        });
        Object.defineProperty(this, "size", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "1024x1024"
        });
        Object.defineProperty(this, "dallEResponseFormat", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "url"
        });
        Object.defineProperty(this, "user", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        var _fields_apiKey, _ref;
        const openAIApiKey = (_ref = (_fields_apiKey = fields === null || fields === void 0 ? void 0 : fields.apiKey) !== null && _fields_apiKey !== void 0 ? _fields_apiKey : fields === null || fields === void 0 ? void 0 : fields.openAIApiKey) !== null && _ref !== void 0 ? _ref : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("OPENAI_API_KEY");
        var _fields_organization;
        const organization = (_fields_organization = fields === null || fields === void 0 ? void 0 : fields.organization) !== null && _fields_organization !== void 0 ? _fields_organization : (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$utils$2f$env$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["getEnvironmentVariable"])("OPENAI_ORGANIZATION");
        const clientConfig = {
            apiKey: openAIApiKey,
            organization,
            dangerouslyAllowBrowser: true,
            baseURL: fields === null || fields === void 0 ? void 0 : fields.baseUrl
        };
        this.client = new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$client$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__["OpenAI"](clientConfig);
        var _fields_model, _ref1;
        this.model = (_ref1 = (_fields_model = fields === null || fields === void 0 ? void 0 : fields.model) !== null && _fields_model !== void 0 ? _fields_model : fields === null || fields === void 0 ? void 0 : fields.modelName) !== null && _ref1 !== void 0 ? _ref1 : this.model;
        var _fields_style;
        this.style = (_fields_style = fields === null || fields === void 0 ? void 0 : fields.style) !== null && _fields_style !== void 0 ? _fields_style : this.style;
        var _fields_quality;
        this.quality = (_fields_quality = fields === null || fields === void 0 ? void 0 : fields.quality) !== null && _fields_quality !== void 0 ? _fields_quality : this.quality;
        var _fields_n;
        this.n = (_fields_n = fields === null || fields === void 0 ? void 0 : fields.n) !== null && _fields_n !== void 0 ? _fields_n : this.n;
        var _fields_size;
        this.size = (_fields_size = fields === null || fields === void 0 ? void 0 : fields.size) !== null && _fields_size !== void 0 ? _fields_size : this.size;
        var _fields_dallEResponseFormat;
        this.dallEResponseFormat = (_fields_dallEResponseFormat = fields === null || fields === void 0 ? void 0 : fields.dallEResponseFormat) !== null && _fields_dallEResponseFormat !== void 0 ? _fields_dallEResponseFormat : this.dallEResponseFormat;
        this.user = fields === null || fields === void 0 ? void 0 : fields.user;
    }
}
Object.defineProperty(DallEAPIWrapper, "toolName", {
    enumerable: true,
    configurable: true,
    writable: true,
    value: "dalle_api_wrapper"
});
}),
"[project]/node_modules/@langchain/openai/dist/tools/index.js [app-client] (ecmascript) <locals>", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$tools$2f$dalle$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/tools/dalle.js [app-client] (ecmascript)");
;
}),
"[project]/node_modules/@langchain/openai/dist/utils/prompts.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([
    "convertPromptToOpenAI",
    ()=>convertPromptToOpenAI
]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$chat_models$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/chat_models.js [app-client] (ecmascript)");
;
function convertPromptToOpenAI(formattedPrompt) {
    const messages = formattedPrompt.toChatMessages();
    return {
        messages: (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$chat_models$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["_convertMessagesToOpenAIParams"])(messages)
    };
}
}),
"[project]/node_modules/@langchain/openai/dist/tools/custom.js [app-client] (ecmascript)", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([
    "customTool",
    ()=>customTool
]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$runnables$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/runnables.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$runnables$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/runnables/index.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$singletons$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/singletons.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$singletons$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/singletons/index.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$tools$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/tools.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$tools$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/core/dist/tools/index.js [app-client] (ecmascript)");
;
;
;
function customTool(func, fields) {
    return new __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$tools$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["DynamicTool"]({
        ...fields,
        description: "",
        metadata: {
            customTool: fields
        },
        func: async (input, runManager, config)=>new Promise((resolve, reject)=>{
                const childConfig = (0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$runnables$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["patchConfig"])(config, {
                    callbacks: runManager === null || runManager === void 0 ? void 0 : runManager.getChild()
                });
                void __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$singletons$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["AsyncLocalStorageProviderSingleton"].runWithConfig((0, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$core$2f$dist$2f$runnables$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["pickRunnableConfigKeys"])(childConfig), async ()=>{
                    try {
                        resolve(func(input, childConfig));
                    } catch (e) {
                        reject(e);
                    }
                });
            })
    });
}
}),
"[project]/node_modules/@langchain/openai/dist/index.js [app-client] (ecmascript) <locals>", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$openai$2f$index$2e$mjs__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/openai/index.mjs [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$chat_models$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/chat_models.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$azure$2f$chat_models$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/azure/chat_models.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$llms$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/llms.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$azure$2f$llms$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/azure/llms.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$azure$2f$embeddings$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/azure/embeddings.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$embeddings$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/embeddings.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$types$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/types.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$openai$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/openai.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$azure$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/azure.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$tools$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/tools/index.js [app-client] (ecmascript) <locals>");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$utils$2f$prompts$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/utils/prompts.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$tools$2f$custom$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/tools/custom.js [app-client] (ecmascript)");
;
;
;
;
;
;
;
;
;
;
;
;
;
}),
"[project]/node_modules/@langchain/openai/index.js [app-client] (ecmascript) <locals>", ((__turbopack_context__) => {
"use strict";

__turbopack_context__.s([]);
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f40$langchain$2f$openai$2f$dist$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__$3c$locals$3e$__ = __turbopack_context__.i("[project]/node_modules/@langchain/openai/dist/index.js [app-client] (ecmascript) <locals>");
;
}),
]);

//# sourceMappingURL=node_modules_%40langchain_openai_9870cc42._.js.map